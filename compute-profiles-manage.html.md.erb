---
title: Creating and Managing Compute Profiles with the CLI (vSphere)
owner: TKGI
---

This topic describes how to use the the `tkg` CLI to create and manage compute profiles in
<%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) on vSphere.

For more information on how to use compute profiles, see
[Using Compute Profiles (vSphere)](./compute-profiles-use.html).

<%= vars.k8s_runtime_abbr %> versions prior to v1.9 used a different format for compute profiles, and these older profiles cannot be managed with the `tkg` CLI.
For information about pre-v1.9 compute profiles, see [Using Compute Profiles](https://docs.pivotal.io/tkgi/1-8/compute-profiles.html) in the <%= vars.k8s_runtime_abbr %> v1.8 documentation.

## <a id='about'></a>About Compute Profiles

A compute profile enables cluster administrators, `pks.clusters.admin`, to override the default settings defined by a plan.

Using a compute profile, cluster administrators can customize the following:

* Compute resources such as CPU, memory, ephemeral disk, and persistent disk for Kubernetes control plane and worker nodes
* vSphere resources for Kubernetes control plane and worker nodes

<p class="note"><strong>Note</strong>: A compute profile overrides only the settings that you specify in the profile definition. The rest of the cluster's configuration is inherited from the plan.</p>

After you create a compute profile, cluster managers can apply it to one or more Kubernetes clusters.


## <a id="create"></a> Create a Compute Profile

To create a compute profile in <%= vars.k8s_runtime_abbr %>, a
cluster administrator must:

1. Define a compute profile in a JSON configuration file.
See [Compute Profile Format](#format) and
[Compute Profile Parameters](#params) below.

1. Use the <%= vars.k8s_runtime_abbr %> CLI to define the compute profile
within <%= vars.k8s_runtime_abbr %>. See
[The create-compute-profile Command](#run-create) below.

### <a id="format"></a> Compute Profile Format

To create a compute profile, a cluster administrator must first define it as a JSON file.

Below is an example compute profile that defines compute resources for
Kubernetes control plane nodes in the `control_plane` block, two node pools for
workers in the `node_pools` block, and one availability zone (AZ) within vSphere infrastructure
in the `azs` block. For more examples,
see [Compute Profile Examples](#examples) below.

<p class="note"><strong>Note:</strong> This example compute profile is for
illustration purposes only.</p>

```
{
  "name": "test-compute-profile",
  "description": "A profile for the east datacenter with heterogeneous workers",
  "parameters": {
    "azs": [{
      "name": "east",
      "cpi": "056bf5a3aea9c5c79f62",
      "cloud_properties": {
        "datacenters": [{
          "name": "east",
          "clusters": [{
            "cluster-0": {
              "host_group": {
                "drs_rule": "MUST",
                "name": "cluster-0-hg"
              }
            },
            "resource_pool": "az-2"
          }]
        }]
      }
    }],
    "cluster_customization": {
      "control_plane": {
        "cpu": 4,
        "memory_in_mb": 16384,
        "ephemeral_disk_in_mb": 32768,
        "az_names": ["east"],
        "instances": 3
      },
      "node_pools": [{
          "name": "x-large",
          "cpu": 4,
          "memory_in_mb": 8192,
          "ephemeral_disk_in_mb": 32768,
          "az_names": ["east"],
          "instances": 3,
          "max_worker_instances": 25
        },
        {
          "name": "small",
          "cpu": 1,
          "memory_in_mb": 2048,
          "ephemeral_disk_in_mb": 8192,
          "az_names": ["east"],
          "instances": 5,
          "max_worker_instances": 30
        }
      ]
    }
  }
}
```

For information about all of the parameters that you can specify in a compute
profile, see [Compute Profile Parameters](#params) below.

### <a id="params"></a> Compute Profile Parameters

The compute profile JSON configuration file includes the following top-level
properties:

<table>
  <tr>
    <th width="29%">Property</th>
    <th>Type</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><code>name</code></td>
    <td>String</td>
    <td>(<strong>Required</strong>) Name of the compute profile. You use this name when managing the
    compute profile or assigning the profile to a Kubernetes cluster through
    the TKGI CLI.</td>
  </tr>
  <tr>
    <td><code>description</code></td>
    <td>String</td>
    <td>(<strong>Required</strong>) Description of the compute profile.</td>
  </tr>
  <tr>
    <td><code>parameters</code></td>
    <td>Object</td>
    <td><strong>(Required)</strong> Set of properties defining the main body of the compute profile such as <code>azs</code> and
    <code>cluster_customization</code>.</td>
  </tr>
  <tr>
    <td><code>azs</code></td>
    <td>Array</td>
    <td>(<strong>Optional</strong>) Properties defining AZ name, CPI, and <code>cloud_properties</code> settings.
    See <a href="#cloud-properties">azs Block</a> below.</td>
  </tr>
  <tr>
    <td><code>cluster_customization</code></td>
    <td>Object</td>
    <td>(<strong>Optional</strong>) Properties defining <a href="#control-plane"><code>control_plane</code></a> and <a href="#node-pools"><code>node_pools</code></a>.</td>
  </tr>
</table>

When you create a compute profile, only the `name`, `description`, and `parameters` properties are required. For example, you can choose to define only node pools for worker nodes in your compute profile.


#### <a id="cloud-properties"></a>`azs` Block (vSphere with NSX-T Only)

This optional block defines where Kubernetes control plane and worker nodes are created
within your vSphere infrastructure. You can define one or more AZs.

If you define the `azs` block, do not use the `persistent_disk_in_mb`
parameter in `cluster_customization`. You can define only one of the two in the same profile.

<table>
  <tr>
    <th width="28%">Property</th>
    <th>Type</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><code>name</code></td>
    <td>String</td>
    <td>Name for the AZ where you want to deploy cluster VMs. For example,
      <code>east</code>.</td>
  </tr>
  <tr>
    <td><code>cpi</code></td>
    <td>String</td>
    <td>BOSH CPI ID of your <%= vars.k8s_runtime_abbr %> deployment. For instructions on how to obtain the ID, see <a href="#retrieve-cpi-id">Retrieve the BOSH CPI ID</a>. For example,
      <code>abc012abc345abc567de</code>.</td>
  </tr>
  <tr>
    <td><code>cloud_properties</code></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td><code>datacenters</code></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td><code>name</code></td>
    <td>String</td>
    <td>Name of your datacenter as it appears in Ops Manager and your cloud provider console. For example, <code>east</code>.</td>
  </tr>
  <tr>
    <td><code>clusters</code></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td><code>CLUSTER-NAME</code></td>
    <td>String</td>
    <td>For example, <code>cluster-0</code>.</td>
  </tr>
  <tr>
    <td><code>host_group</code></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td><code>drs_rule</code></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td><code>name</code></td>
    <td>String</td>
    <td></td>
  </tr>
  <tr>
    <td><code>resource_pool</code></td>
    <td>String</td>
    <td>Name of the resource pool where you want to deploy your cluster.</td>
  </tr>
</table>

####<a id="retrieve-cpi-id"></a> Retrieve the BOSH CPI ID

Use the following procedure to retrieve the BOSH CPI ID for your
<%= vars.product_short %> deployment.

1. Locate the credentials that were used to import the Ops Manager .ova or .ovf file into your virtualization system. You configured these credentials when you installed Ops Manager.
    <p class="note"><strong>Note</strong>: If you lose your credentials, you must shut down the Ops Manager VM in the vSphere UI and reset the password. See <a href="https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.security.doc/GUID-4BDBF79A-6C16-43B0-B0B1-637BF5516112.html">vCenter Password Requirements and Lockout Behavior</a> in the vSphere documentation for more information.</p>

1. From a command line, run the following command to SSH into the Ops Manager VM:

    ```
    ssh ubuntu@OPS-MANAGER-FQDN
    ```
    Where `OPS-MANAGER-FQDN` is the fully qualified domain name (FQDN) of Ops Manager.

1. When prompted, enter the password that you configured during the .ova deployment
into vCenter. For example:
  <pre class="terminal">
  $ ssh ubuntu&#64;my-opsmanager-fqdn.example.com
  Password: ***********
  </pre>

1. Run `bosh cpi-config` to locate the Cloud Provider Interface (CPI) name for your deployment. For example:

    <pre class="terminal">$ bosh cpi-config
    Using environment 'BOSH-DIRECTOR-IP' as client 'ops_manager'
    cpis:
    &#45; migrated_from:
      &#45; name: ""
      name: YOUR-CPI-NAME
    </pre>
    For more information about running BOSH commands in your <%= vars.product_short %> deployment, see [Using BOSH Diagnostic Commands in <%= vars.product_short %>](diagnostic-tools.html).

#### <a id="control-plane"></a>`control_plane` Block

This optional block defines properties for Kubernetes control plane node instances.

When defining the `control_plane` block,
you must specify either `cpu`, `memory_in_mb`, and
`ephemeral_disk_in_mb` or none of the three.

<table>
  <tr>
    <th width="30%">Property</th>
    <th>Type</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><code>cpu</code></td>
    <td>Integer</td>
    <td>CPU count for control plane instances.</td>
  </tr>
  <tr>
    <td><code>memory_in_mb</code></td>
    <td>Integer</td>
    <td>RAM for control plane instances.</td>
  </tr>
  <tr>
    <td><code>ephemeral_disk_in_mb</code></td>
    <td>Integer</td>
    <td>Ephemeral disk for control plane instances.</td>
  </tr>
  <tr>
    <td><code>persistent_disk_in_mb</code></td>
    <td>Integer</td>
    <td>Persistent disk for control plane instances. Do not specify this parameter if you intend to define the <code>azs</code> block in the compute profile.</td>
  </tr>
  <tr>
    <td><code>az_names</code></td>
    <td>Array</td>
    <td>AZs in which you want control plane instances to run.</td>
  </tr>
  <tr>
    <td><code>instances</code></td>
    <td>Integer</td>
    <td>Number of control plane instances. Enter <code>1</code>, <code>3</code>, or <code>5</code>.</td>
  </tr>
</table>

Do not assign the `name` property to this block.

#### <a id="node-pools"></a>`node_pools` Block

This optional block defines properties for Kubernetes worker nodes.
You can define one or more node pools.

When defining the `node_pools` block,
you must specify either `cpu`, `memory_in_mb`, and
`ephemeral_disk_in_mb` or none of the three.

<table>
  <tr>
    <th width="30%">Property</th>
    <th>Type</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><code>name</code></td>
    <td>String</td>
    <td>Unique Name of the node pool.</td>
  </tr>
  <tr>
    <td><code>cpu</code></td>
    <td>Integer</td>
    <td>CPU count for worker node instances.</td>
  </tr>
  <tr>
    <td><code>memory_in_mb</code></td>
    <td>Integer</td>
    <td>RAM for worker node instances.</td>
  </tr>
  <tr>
    <td><code>ephemeral_disk_in_mb</code></td>
    <td>Integer</td>
    <td>Ephemeral disk for worker node instances.</td>
  </tr>
  <tr>
    <td><code>persistent_disk_in_mb</code></td>
    <td>Integer</td>
    <td>Persistent disk for worker node instances.
    Do not specify this parameter if you intend to define the <code>azs</code> block in the compute profile.</td>
  </tr>
  <tr>
    <td><code>az_names</code></td>
    <td>Array</td>
    <td>AZs in which you want worker node instances to run.</td>
  </tr>
  <tr>
    <td><code>instances</code></td>
    <td>Integer</td>
    <td>Number of worker node instances.</td>
  </tr>
  <tr>
    <td><code>max_worker_instances</code></td>
    <td>Integer</td>
    <td>Maximum number of worker node instances for the node pool.</td>
  </tr>
</table>

### <a id='examples'></a> Compute Profile Examples

See the table below for examples of compute profiles.

Example                                   | Description
----------------------------------------------------------------|------------------------------------------------------------------------------
[Custom Nodes](#custom-nodes)        | Define custom compute resources for Kubernetes control plane and worker nodes.
[Worker Node Pools](#node-pools)       | Define multiple pools of worker nodes.
[Custom AZs](#custom-azs)       | Define AZs for Kubernetes control plane nodes and worker node pools dynamically.

#### <a id='custom-nodes'></a> Custom Nodes

The example below defines compute resources for control plane nodes
and one node pool for workers:

```
{
  "name": "custom-nodes-compute-profile",
  "description": "custom-nodes-compute-profile",
  "parameters": {
    "cluster_customization": {
      "control_plane": {
        "cpu": 2,
        "memory_in_mb": 4096,
        "ephemeral_disk_in_mb": 16384,
        "persistent_disk_in_mb": 16384,
        "instances": 3
      },
      "node_pools": [{
        "cpu": 2,
        "memory_in_mb": 4096,
        "ephemeral_disk_in_mb": 16384,
        "persistent_disk_in_mb": 16384,
        "name": "tiny-1",
        "instances": 5,
        "max_worker_instances": 10
      }]
    }
  }
}
```

#### <a id='node-pools'></a> Node Pools

Cluster administrators can define pools of worker nodes with different compute resources. Cluster managers then apply the compute profile to one or more cluster. This enables cluster managers to schedule workloads with different compute requirements on a single cluster.

The example below defines compute resources for two worker node pools:

```
{
  "name": "custom-node-pools-compute-profile",
  "description": "custom-node-pools-compute-profile",
  "parameters": {
    "cluster_customization": {
      "node_pools": [{
          "cpu": 2,
          "memory_in_mb": 4096,
          "ephemeral_disk_in_mb": 16384,
          "persistent_disk_in_mb": 16384,
          "name": "tiny-1",
          "instances": 5
        },
        {
          "cpu": 4,
          "memory_in_mb": 4096,
          "ephemeral_disk_in_mb": 32768,
          "name": "medium-2",
          "instances": 1,
          "max_worker_instances": 5
        }
      ]
    }
  }
}
```

#### <a id='custom-azs'></a> Custom AZs (vSphere with NSX-T Only)

Cluster administrators can define AZs in a compute profile instead of adding new AZs in the BOSH Director tile. Cluster managers then use it to specify AZs for a cluster dynamically. As a result, you do not need to make AZ changes to each <%= vars.k8s_runtime_abbr %> plan and the overall impact to the
<%= vars.k8s_runtime_abbr %> control plane is smaller.

The example below defines an AZ, `east`, in the `azs` block, which is then
referenced in the `cluster_customization` block.

```
{
  "name": "dc-east-single-node-pool",
  "description": "A profile for the east datacenter with a single node pool.",
  "parameters": {
    "azs": [{
      "name": "east",
      "cpi": "056bf5a3aea9c5c79f62",
      "cloud_properties": {
        "datacenters": [{
          "name": "east",
          "clusters": [{
            "cluster-0": {
              "host_group": {
                "drs_rule": "MUST",
                "name": "cluster-0-hg"
              }
            },
            "resource_pool": "az-2"
          }]
        }]
      }
    }],
    "cluster_customization": {
      "control_plane": {
        "cpu": 4,
        "memory_in_mb": 16384,
        "ephemeral_disk_in_mb": 32768,
        "az_names": ["east"],
        "instances": 3
      },
      "node_pools": [{
        "name": "x-large",
        "cpu": 4,
        "memory_in_mb": 8192,
        "ephemeral_disk_in_mb": 32768,
        "az_names": ["east"],
        "instances": 3,
        "max_worker_instances": 25
      }]
    }
  }
}
```

### <a id="run-create"></a> The create-compute-profile Command

After a compute profile is defined in a JSON file as described in [Compute Profile Format](#format),
a cluster administrator can create the compute profile by running the following <%= vars.k8s_runtime_abbr %> CLI command:

```
tkgi create-compute-profile PATH-TO-YOUR-COMPUTE-PROFILE-CONFIGURATION
```
Where `PATH-TO-YOUR-COMPUTE-PROFILE-CONFIGURATION` is the path to the JSON file
you created when defining the compute profile.

For example:

<pre class="terminal">
$ tkgi create-compute-profile large-workers.json

Compute profile large-workers successfully created
</pre>

Only cluster administrators, `pks.clusters.admin`, can create compute profiles.
If a cluster manager `pks.clusters.manage` or read-only admin `pks.clusters.admin-read-only` attempts to create a compute profile, the following error occurs:

```
You do not have enough privileges to perform this action. Please contact the <%= vars.k8s_runtime_abbr %> administrator.
```

After an administrator creates a compute profile, cluster managers can create clusters with it or assign it to existing clusters.
For more information, see the [Using Compute Profiles (vSphere)](./compute-profiles-use.html) topic.

## <a id="manage"></a> Manage Compute Profiles

<%= vars.k8s_runtime_abbr %> administrators can delete compute profiles.
Administrators can also perform the same operations that cluster managers use to list compute profiles and manage how clusters use them.

<p class="note warning"><strong>Warning:</strong> These commands do not work for compute profiles created using the <%= vars.k8s_runtime_abbr %> API in <%= vars.k8s_runtime_abbr %> v1.8 or earlier.</p>


### <a id="view"></a> View a Compute Profile

To view details about a compute profile, run the following command:

```
tkgi compute-profile COMPUTE-PROFILE-NAME
```

Where `COMPUTE-PROFILE-NAME` is the name of the compute profile you want to
view.

For example:

<pre class="terminal">
tkgi compute-profile test-compute-profile

Name:         test-compute-profile
Description:  test-compute-profile
Parameters:
  Cluster Customization:
    Control Plane:
      Name:
      Instances:            3
      CPU:                  2
      Memory (Mb):          4096
      Ephemeral Disk (Mb):  16384
    Node:
      Name:                 tiny-1
      Instances:            5
      CPU:                  2
      Memory (Mb):          4096
      Ephemeral Disk (Mb):  16384
    Node:
      Name:                 medium-2
      Instances:            1
      CPU:                  4
      Memory (Mb):          4096
      Ephemeral Disk (Mb):  32768
</pre>

### <a id="delete"></a> Delete a Compute Profile

To delete a compute profile, run the following command:

```
tkgi delete-compute-profile COMPUTE-PROFILE-NAME
```

Where `COMPUTE-PROFILE-NAME` is the name of the compute profile you want to delete.

For example:
<pre class="terminal">
tkgi delete-compute-profile test-compute-profile-8

Are you sure you want to delete the compute profile test-compute-profile-8? (y/n): y
Deletion of test-compute-profile-8 completed
</pre>

Limitations:

- You cannot delete a compute profile that is in use.
The profile must be disassociated from all clusters, or all associated clusters must be deleted.

- Only cluster administrators, `pks.clusters.admin`, can delete compute profiles.
If a cluster manager `pks.clusters.manage` or read-only admin `pks.clusters.admin-read-only` attempts to delete a compute profile, the following error occurs:

	```
	You do not have enough privileges to perform this action. Please contact the <%= vars.k8s_runtime_abbr %> administrator.
	```

### <a id="user-ops"></a> Cluster Manager Operations

The following sections link to operations that both <%= vars.k8s_runtime_abbr %> administrators and cluster managers can perform on compute profiles,
documented in the [Using Compute Profiles (vSphere)](compute-profiles-use.html) topic.

* [List Compute Profiles](compute-profiles-use.html#list-profiles)
* [Create a Cluster with a Compute Profile](compute-profiles-use.html#create)
* [Assign a Compute Profile to an Existing Cluster](compute-profiles-use.html#assign)
  - This operation can assign a compute profile to a cluster that does not have one, or change a cluster's existing profile.
  - You cannot change a cluster's compute profile to move its nodes to different AZs.

## <a id="vs-plans"></a> Compute Profiles vs. Plans

As with plans defined in <%= vars.k8s_runtime_abbr %> tile **Plans** panes,
compute profiles let <%= vars.k8s_runtime_abbr %> admins define cluster resource choices for developers using Kubernetes.

Compute profiles offer more granular control over cluster topology and node sizing than plans do.
For example, compute profiles can define heterogenous clusters with different
CPU, memory, ephemeral disk, or persistent disk settings for master nodes and worker nodes.

You can also apply a compute profile to specific clusters, overriding the default settings defined by their plan and possibly avoiding the need to create new plans.

You use the <%= vars.k8s_runtime_abbr %> tile to manage plans
and the <%= vars.k8s_runtime_abbr %> CLI to manage compute profiles.
