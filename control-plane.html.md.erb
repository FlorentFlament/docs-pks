---
title: Enterprise PKS Architecture
owner: PKS
---

This topic describes how <%= vars.product_full %> manages the deployment of Kubernetes clusters.

##<a id="overview"></a><%= vars.product_short %> Overview

An <%= vars.product_short %> environment consists of the following components:

* PKS API Load Balancer  
* The PKS Control Plane  
    * <%= vars.control_plane %> VM  
    * <%= vars.control_plane_db %> VM  
    * Ops Manager VM  
    * BOSH VM  
* Workload clusters 
    * Load balancers
    * Kubernetes clusters to host workloads 

<%= vars.product_short %> administrators use the PKS Control Plane to 
deploy and manage Kubernetes clusters.  

The following illustrates the interaction between <%= vars.product_short %> components:  
<br>
<%= image_tag('images/pks-overview.png') %>
<%# Image source: https://docs.google.com/drawings/d/1EJfnaupkCal2_R_eDUWXOt4fiRmNKrUPft3cv4mgDr4/edit %>

Administrators access the PKS Control Plane 
through the PKS Command Line Interface (PKS CLI) installed on their local workstations. 
Within the PKS Control Plane the PKS API and PKS Broker use BOSH to execute the requested cluster management functions. 
For information about the PKS Control Plane, see [PKS Control Plane Overview](#control-plane) below. 
For instructions on installing the PKS CLI, see [Installing the PKS CLI](installing-pks-cli.html).  

Kubernetes deploys and manages workloads on Kubernetes clusters.
Administrators use the  Kubernetes CLI, `kubectl`, to direct Kubernetes 
from their local workstations. 
For information about `kubectl`, see [Overview of kubectl]
(https://kubernetes.io/docs/reference/kubectl/overview/) in the Kubernetes documentation.  

##<a id="cluster-management"></a><a id="control-plane"></a>PKS Control Plane Overview

The PKS Control Plane manages the lifecycle of Kubernetes clusters deployed 
using <%= vars.product_short %>. 

The control plane provides the following via the PKS API:

* View cluster plans
* Create clusters
* View information about clusters
* Obtain credentials to deploy workloads to clusters
* Scale clusters
* Delete clusters
* Create and manage network profiles for VMware NSX-T

In addition, the PKS Control Plane can upgrade all existing clusters using the **Upgrade all clusters** BOSH errand.
For more information, see [Upgrade Kubernetes Clusters](upgrade-pks.html#upgrade-instances) in _Upgrading <%= vars.product_short %>_.

###<a id="architecture"></a>PKS Control Plane Architecture

PKS Control Plane is hosted on a pair VMs, or optionally on a pair of highly available VM groups.
The component hosts the following services:

* User Account and Authentication (UAA)  
* PKS API  
* PKS Broker
* Billing and Telemetry 
* PKS Database 

The following sections describe UAA, PKS API, and PKS Broker services,
the primary services hosted on the <%= vars.control_plane %> VM.

####<a id="uaa"></a>UAA

When a user logs in to or logs out of the PKS API through the PKS CLI, the PKS CLI communicates with UAA to authenticate them.
The PKS API permits only authenticated users to manage Kubernetes clusters.
For more information about authenticating, see [PKS API Authentication](pks-api-auth.html).

UAA must be configured with the appropriate users and user permissions.
For more information, see [Managing <%= vars.product_short %> Users with UAA](manage-users.html).

####<a id="pks-api"></a>PKS API

Through the PKS CLI, users instruct the PKS API service to deploy, scale up, and delete Kubernetes clusters as well as show cluster details and plans.
The PKS API can also write Kubernetes cluster credentials to a local kubeconfig file, which enables users to connect to a cluster through `kubectl`.

On AWS, GCP, and vSphere without NSX-T deployments the PKS CLI communicates with the 
PKS API within the control plane via the PKS API Load Balancer. 
On vSphere with NSX-T deployments the <%= vars.control_plane %> host is accessible via a DNAT rule. 
For information about enabling the PKS API on vSphere with NSX-T, see the 
[Share the PKS API Endpoint](installing-nsx-t.html#retrieve-endpoint) section in 
_Installing <%= vars.product_short %> on vSphere with NSX-T Integration_.

The PKS API sends all cluster management requests, except read-only requests, to the PKS Broker.

####<a id="pks-broker"></a>PKS Broker

When the PKS API receives a request to modify a Kubernetes cluster, it instructs the PKS Broker to make the requested change.

The PKS Broker consists of an [On-Demand Service Broker](https://docs.pivotal.io/svc-sdk/odb/index.html) and a Service Adapter. The PKS Broker generates a BOSH manifest and instructs the BOSH Director to deploy or delete the Kubernetes cluster.

For <%= vars.product_short %> deployments on vSphere with NSX-T, there is an additional component, the <%= vars.product_short %> NSX-T Proxy Broker.
The PKS API communicates with the PKS NSX-T Proxy Broker, which in turn communicates with the NSX Manager to provision the Node Networking resources.
The PKS NSX-T Proxy Broker then forwards the request to the On-Demand Service Broker to deploy the cluster.

####<a id="overview-sql"></a>PKS Database

By default, the PKS Database is hosted on `pks-db`, the single-node <%= vars.control_plane_db %> VM, 
which hosts the MySQL, proxy, and other data-related services.  

PKS Control Plane data is persisted to the <%= vars.control_plane_db %> 
for the following services:

* PKS API
* UAA
* Billing
* Telemetry


####The PKS Control Plane High Availability Mode

The PKS Control Plane can be configured in either Standard or High Availability (Beta) modes.

* In Standard Mode, the the <%= vars.control_plane %> and the <%= vars.control_plane_db %> 
are hosted on a pair of dedicated VMs.
* In High Availability (Beta) Mode, 
the <%= vars.control_plane %> is hosted on two or more VMs in a group, 
and the <%= vars.control_plane_db %> is hosted in a three-node MySQL cluster.

<br>
The following illustrates the interaction between <%= vars.product_short %> components in High Availability Mode:
<%= image_tag('images/pks-overview-ha.png') %>
<%# Image source: https://docs.google.com/drawings/d/1hTTmoBpkcjvZJTwlwsXMy9fn91x3KGfIeM4_dzIrW60/edit %>

#### <a id='windows-ha'></a>Windows Worker-Based Kubernetes Cluster High Availability

The following illustrates the interaction between the 
<%= vars.product_short %> Management Plane and Windows worker-based Kubernetes clusters:
<%= image_tag('images/pks-overview-windows-ha-linux-workers.png') %>
<%# Image source: https://docs.google.com/drawings/d/1ec0T8iZx3P8Uf-dxnl1dLesuvtabTwapXClf2kxZ3qA/edit %>
To configure <%= vars.product_short %> Windows worker-based clusters for high availability, set these fields in the **Plan** pane as described in [Plans](windows-pks-beta.html#plans) in _Configuring Windows Worker-Based Kubernetes Clusters (Beta)_:

* **Enable HA Linux workers**
* **Master/ETCD Node Instances**
* **Worker Node Instances**