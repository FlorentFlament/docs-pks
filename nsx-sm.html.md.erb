---
title: Using NSX-SM with Enterprise PKS (BETA)
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to use NSX-SM with <%= vars.product_short %>.

##<a id='prereqs'></a>Prerequisites

This topic assumes that you have deployed <%= vars.product_short %> and provisioned a Kubernetes cluster. 

##<a id='nsx-sm-about'></a> About NSX-SM

NSX-SM provides a service mesh solution for Kubernetes based on the NSX. For more information, refer to the [NSX-SM blog post](https://blogs.vmware.com/cloudnative/2018/12/10/service-mesh-cna/).

NSX-SM is deployed as a Pod in a Kubernetes cluster using a YAML file. The recommendation is to deploy NSX-SM in an Enterprise PKS foundation with Pod Security Policies (PSPs) enabled. PSP are enabled by selecting the check box in the appropriate Plan on the PKS tile. The instructions below include an example PSP (`psp.yaml`) you can use for this purpose.

##<a id='nsx-sm-procedure'></a> Installing NSX-SM in the Cluster

NSX-SM is provided as a service through VMware Cloud Services. You need to go through the service on-boarding process using the NSX-SM web site.

###<a id='nsx-sm-register'></a> Step 1: Register with VMware Cloud Services

1. Go to the [VMware Cloud Services](https://console-stg.cloud.vmware.com/csp/gateway/portal/api/service-invitations?serviceInvitationLink=/csp/slc/api/service-invitations/c7371b0f-545d-4f7f-828e-b0df8d48861a) website and create an account.

1. Complete the registration process by following the emails you receive.

1. Sign in to the VMware Cloud Services Console.

1. Create a new organization.
  <img src="images/nsxt/nsx-sm-01.png">

1. Select the NSX-SM service offering and add your account to the service.

###<a id='nsx-sm-cluster'></a> Onboard a Kubernetes Cluster

The cluster on-boarding process consists of the following steps:

1. Once the cluster is selected, log on to the NSX-SM web site and select the “ADD NEW” item on the home web page. 

1. Input the desired cluster name. The cluster name is used by NSX-SM to identify a particular cluster. It does not have to match the Enterprise PKS cluster name.

1. Download a yaml file. The yaml file contains the deployment definition of the NSX-SM agent.

1. Deploy the NSX-SM yaml file in the default namespace on the target Kubernetes cluster:

```
kubectl apply -f <yaml file name>
```

###<a id='nsx-sm-istio'></a> Install and Configure Istio

Once the NSX-SM agent is correctly started on a cluster, go back the NSX-SM web site and complete the on-boarding process by clicking on the **Install ISTIO** button in the on-boarding menu. When this operation is completed all the ISTIO components have been installed on the target cluster.

The last step causes the ISTIO CNI plugin to be installed. This plug in allows ISTIO to automatically inject its Envoy "sidecar" container whenever a new POD is started.

Since PSPs are enabled on the cluster, deployment of services require a few additional steps, in particular you must create and use a PSP and grant use of it to a service account. While the additional steps are typical for any cluster with PSPs enabled, the content of the PSP object itself is specific to the needs of **sidecar injection** as requested by ISTIO. Use the following `psp.yaml` file for this purpose. For instructions on how to create the PSP, see [Enabling and Configuring Pod Security Policies](http://docs-pcf-staging.cfapps.io/pks/1-5/pod-security-policy.html).

```
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: nsx-sm-sample-psp
spec:
  allowPrivilegeEscalation: false
  allowedCapabilities:
  - '*'
  fsGroup:
    rule: RunAsAny
  hostIPC: false
  hostNetwork: false
  hostPID: false
  hostPorts:
  - max: 65535
    min: 0
  privileged: false
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - awsElasticBlockStore
  - azureDisk
  - azureFile
  - cephFS
  - configMap
  - csi
  - downwardAPI
  - emptyDir
  - fc
  - flexVolume
  - flocker
  - gcePersistentDisk
  - glusterfs
  - iscsi
  - nfs
  - persistentVolumeClaim
  - projected
  - portworxVolume
  - quobyte
  - rbd
  - scaleIO
  - secret
  - storageos
  - vsphereVolume
```

###<a id='nsx-sm-istio'></a> Known Issue

When a delete cluster command is issued in Enterprise PKS, the system runs an errand to clean up the pods currently running in the cluster. ISTIO installs a few pods that have a Pod Disruption Budget that conflict with the Enterprise PKS cleanup errand. This means that the errand will run for a long time. 

In Enterprise PKS v1.5 we allow the user to select a timeout for Pod Disruption Budget, therefore the errand will run up to that timeout. Prior to v1.5 the timeout was very long (approximately 24 hours) and it looked like the deleting process was hanging forever. 

To avoid this problem we suggest that you first try removing the on-boarded cluster from the NSX-SM web UI. To perform this operation log on to the NSX-SM web UI and click on the name of cluster you want to remove. Near the top right corner click on: “REMOVE CLUSTER”. If this operation is successful the cluster can be safely deleted.

If the operation is not successful, run the following command on the cluster before attempting to delete it through Enterprise PKS: 

```
kubectl delete namespace istio-system
```
