---
title: Installing and Configuring NSX-T Data Center v3.0 for Tanzu Kubernetes Grid Integrated Edition
owner: TKGI-NSXT
---

This topic provides instructions for installing and configuring NSX-T Data Center v3.0 for use with <%= vars.product_full %> on vSphere.

##<a id='nsxt30-prereqs'></a> Prerequisites for Installing NSX-T Data Center v3.0 for <%= vars.product_short %>

To perform a new installation of NSX-T Data Center for <%= vars.product_short %>, complete the following steps in the order presented.

1. Read the [Release Notes](./release-notes.html) for the target <%= vars.k8s_runtime_abbr %> version you are installing and verify NSX-T 3.0 support.

1. Read the topics in the [Preparing to Install <%= vars.product_short %> on vSphere with NSX-T Data Center](./vsphere-nsxt-index-prepare.html) section of the documentation.

##<a id='nsxt30-mgr1'></a> Deploy NSX-T Manager 1

Deploy the NSX-T Manager OVA in vSphere.

1. Select deploy OVF template.
1. For the size select MEDIUM.
1. For Virtual Disk Format, choose Thin Provision.
1. For Network1, enter the VLAN managment network, such as `PG-MGMT-VLAN-1548`.
1. Enter the hostname, such as `nsx-manager-1`.
1. Enter the rolename, such as `NSX Manager`.
1. Enter the Gateway, such as `10.173.62.253`.
1. Enter a public IP address, such as `10.173.62.44`.
1. Enter the Netmask, such as `255.255.255.0`.
1. Enter the DNS server, such as `10.172.40.1`.
1. Enter the `NTP server`, such as `10.113.60.176`.
1. Select Enable SSH so it is checked.
1. Select Allow SSH root login so it is checked.
1. Click Finish. NSX-T manager 1 is deployed.
1. Monitor the deploymen using the Tasks pane.
1. When the deployment completes, select the VM and power it on.
1. Access the NSX-T Manager 1 web console by navigating to the URL, such as: `https://10.173.62.44/`.
 
##<a id='nsxt30-compute-mgr'></a> Create a Compute Manager

A compute manager is required for NSX-T environments with multiple NSX-T Managers. We configure vCenter Server as the Compute Manager.

1. Go to System > Appliances.
1. To add a second NSX-T manager, we need first to create a Compute Manager.
1. Go to Compute Manager:
1. Click on Add
1. Enter a Name, such as vCenter.
1. Enter an IP, such as 10.173.62.43.
1. Enter a username, such as administrator@vsphere.local.
1. Select Enable Trust: yes
1. Click Add.
1. Verify that the Compute Manager is added.

##<a id='nsxt30-mgr2'></a> Deploy NSX-T Managers 2 and 3

1. Select Add NSX Appliance.
1. Enter a hostname, such as `nsx-manager-2`.
1. Enter the Management IP/netmask, such as `10.173.62.45/24`.
1. Enter the Gateway, such as `10.173.62.253`.
1. For the Node size, choose `medium`.
1. For the Compute Manager, select `vCenter`.
1. For the Compute Cluster, enter `MANAGEMENT-cluster`.
1. For the Datastore, select the datastore, such as `datastore2`.
1. For the Virtual Disk Format, select `thin provision`.
1. For the Network, select the VLAN management network, such as `PG-MGMT-VLAN-1548`.
1. Select Enable SSH.
1. Select Enable root access.
1. Enter a strong password.
1. Click Install Appliance.
1. Verify that the NSX-T Manager 2 appliance is added.
1. Repeat the same operation for NSX-T Manager 3 that you performed for NSX-T Manager 2.
1. Use a different public IP address.
1. Verify that the NSX-T Manager 3 appliance is added.

##<a id='nsxt30-mgmt-vip'></a> Configure the NSX-T Management VIP

1. Click the Set Virtual IP button.
1. Enter a Virtual IP address, such as `10.173.62.47`.
1. Verify that the VIP is added.. 
1. Access the NSX-T Management console using the VIP, such as `https://10.173.62.47/login.jsp`.

##<a id='nsxt30-mgmt-ssl'></a> Generate and Register the NSX-T Management SSL Certificate and Private Key
 
A SSL certificate is automatically created for each NSX-T Manager. You can verify this by SSHing to one of the NSX Manager nodes using the command `get certificate cluster`. For example:

```
nsx-manager-1> get certificate cluster
```

The issuer, however, is `CN=nsx-manager-1`, which means the cluster certificate is linked to a particular NSX Manager, in this case NSX-T Manager 1. You need to generate a new SSL certficate that uses the NSX-T Management VIP address so that the the cluster certificate contains CN=VIP. If you go to System > Certificates, you will see there is no certificate automatically created for NSX-T manager VIP.

NOTE: You must run these steps on a host where OpenSSL is installed. The steps assume that you perform them on a Linux host. If you are using a different OS, you will need to adjust the steps accordingly.

###<a id='nsxt30-mgmt-cert-gen'></a> Generate the SSL Certifciate and Private Key

1. Create a certificate signing request file named `nsx-cert.cnf` and populate it with the contents below. Replace the IP addresses for `commonName` and `DNS.1` with the VIP address for you NSX-T Management plane.

```
nsx-cert.cnf
[ req ]
default_bits = 2048
distinguished_name = req_distinguished_name
req_extensions = req_ext
prompt = no
[ req_distinguished_name ]
countryName = US
stateOrProvinceName = California
localityName = CA
organizationName = NSX
commonName = 10.173.62.47 		# REPLACE
[ req_ext ]
subjectAltName = @alt_names
[alt_names]
DNS.1 = 10.173.62.47			# REPLACE
```

1. Create the following environment variables. Replace the IP address with your VIP.

```
export NSX_MANAGER_IP_ADDRESS=10.173.62.47
export NSX_MANAGER_COMMONNAME=10.173.62.47
```

1. Generate the SSL certificate and private key.

```
openssl req -newkey rsa:2048 -x509 -nodes \
> -keyout nsx.key -new -out nsx.crt -subj /CN=$NSX_MANAGER_COMMONNAME \
> -reqexts SAN -extensions SAN -config <(cat ./nsx-cert.cnf \
> <(printf "[SAN]\nsubjectAltName=DNS:$NSX_MANAGER_COMMONNAME,IP:$NSX_MANAGER_IP_ADDRESS")) -sha256 -days 1000
-----

1. Verify that you see the following:

```
Generating a 2048 bit RSA private key
...............+++
................+++
writing new private key to 'nsx.key'
```

1. Verify certificate and key generation by running the `ls` command. You should see the following 3 files.

```
nsx-cert.cnf  nsx.crt  nsx.key
```

1. Run the following command to verify the certificate and private key. 

```
openssl x509 -in nsx.crt -text -noout
```

You should see that the comman name (CN) and Subject Alternative Name are both the VIP address. For example:

```
Subject: CN = 10.173.62.47
Subject Alternative Name: 
	DNS:10.173.62.47, IP Address:10.173.62.47
```

###<a id='nsxt30-mgmt-cert-import'></a> Import the SSL Certificate and Private Key to the NSX-T Management Console

1. Import certificate and private key to NSX-T by navigating to the System > Certificates page.
 
1. Select import certificate.

1. Enter a Name, such as `CERT-NSX-T-VIP`.

1. Copy/paste the Certificate Contents from the `nsx.crt` file. If you copy the `nsx.crt` file to your local machine, you can import the certificate by clicking Browse and selecting it.

1. Copy/paste the Private Key from the `nsx.key` file. If you copy the `nsx.key` file to your local machine, you can import the certificate by clicking Browse and selecting it.

1. For the Service Certificate option, select No.

1. Click Import.

1. Verify that you see the certificate in the list of Certificates.

###<a id='nsxt30-mgmt-cert-reg'></a> Register the SSL Certificate and Private Key with the NSX API Server

1. Retrieve the UDID of the certificate `CERT-NSX-T-VIP` from teh NSX-T Management console > Certificates screen. 

1. Copy this UUID to the clipboard, such as `170a6d52-5c61-4fef-a9e0-09c6229fe833`.

1. Create the following environment variables. Replace the IP address with your VIP address.

```
export NSX_MANAGER_IP_ADDRESS=10.173.62.47
export CERTIFICATE_ID=170a6d52-5c61-4fef-a9e0-09c6229fe833
```

1. Post the certificate to the NSX-T Manager API.

```
curl --insecure -u admin:'VMware1!VMware1!' -X POST "https://$NSX_MANAGER_IP_ADDRESS/api/v1/cluster/api-certificate?action=set_cluster_certificate&certificate_id=$CERTIFICATE_ID"
{
  "certificate_id": "170a6d52-5c61-4fef-a9e0-09c6229fe833"
}
```

1. Verfiy by SSHing to one of the NSX-T Management nodes and running the `get certificate cluster` command. The certificate that is returned should match the generated one.

```
nsx-manager-1> get certificate cluster
```

##<a id='nsxt30-mgmt-vip'></a> Complete the NSX-T Managment

###<a id='nsxt30-mgmt-enable-adv'></a> Enable the NSX-T Manager Inferface

The **Manager** interface provides the advanced UI for networking and security that is used for configuring NSX-T object for TKGI. Do **NOT** use the **Policy** interface for TKGI objects.

1. Go to System > User Interface Settings.
 
1. Select Toggle Visibility: visible to all users.

1. Select Default Mode: Manager.

1. Click Save.

1. Verify that you see the **Manager** option in the upper right of the console next to **Policy**. 

###<a id='nsxt30-mgmt-license'></a> Add the NSX-T Manager License

1. Go to System > Licences.
 
1. Add the license there.

1. Verify. You should see that the license is added: NSX Data Center Advanced (CPU). If the license is not added properly, you will receive an error message when you try to deploy a Edge Node VM.

##<a id='nsxt30-ip-pools'></a> Create IP Pools

Create IP Pool for VTEP.

1. Select the Manager interface (upper right).

1. Go to Networking > IP Address Pool.

1. Select Add.
 
1. Enter a Name, such as `TEP-IP-POOL`.

1. Enter an IP range, such as `192.23.213.1 - 192.23.213.10`.
 
1. Enter a CIDR address, such as `192.23.213.0/24`.

1. Click Add.

1. Verify that the pool is added.

##<a id='nsxt30-tzs'></a> Create Transport Zones

You need two transport zones, an overlay and a VLAN. By default NSX-T creates two transport zones for you: `nsx-overlay-transportzone` and `nsx-vlan-transportzone`. However, the default TZs cannot be used by TKGI. If you do use them, you will recieve an `pks-nsx-t-osb-proxy` BOSH error when you try to deploy TKGI.

Thus, create two new transport zones:
- `tz-overlay` (switch name: `switch-overlay`), and
- `tz-vlan` (switch name: `switch-vlan`)

1. Go to System > Fabric > Transport Zone.
 
1. Click Add.

1. Enter a Name, such as `tz-overlay`.

1. Enter a switch name, such as `switch-overlay`.

1. For the Traffic Type, select `Overlay`.

1. Click Add.

1. Verify that you see the newly created TZ named `tz-overlay` in the list. 
 
1. Click Add.

1. Enter a name, such as `tz-vlan`.

1. Enter a switch name, such as `switch-vlan`.

1. For the Traffic Type, select `VLAN.

1. Click Add.

1. Verify that you see the newly created TZ named `tz-vlan` in the list. 

##<a id='nsxt30-esxi-vswitch'></a> Configure vSphere Networking for ESXi Hosts

In this section you configure the vSphere networking and port groups for ESXi hosts (the vSwitch). If you have created separate vSphere clusters for Management and Compute, perform this operation on each ESXi host in the Management cluster. If you have not created separate vSphere clusters, perform this operation on each ESXi host in the cluster.

The following instructions describe how to configure a vSphere Virtual Standard vSwitch (VSS). For production environments it is recommended that you configure a Virtual Distributed vSwitch (VDS). You configure the VDS from the vCenter **Networking** tab then add the ESXi hosts to the VDS. The configuration settings for the VDS are similar to the VSS configuration described below. For instructions on configuring the VDS, see <a href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.networking.doc/GUID-D21B3241-0AC9-437C-80B1-0C8043CC1D7D.html">Create a vSphere Distributed Switch</a> in the vSphere 7 documentation.

TKGI v1.8 uses VSS or VDS for non-NSX-T integration on vSphere, and N-VDS for integration with NSX-T. To summarize, TKGI v1.8 supports the following networking model:
- 1 VDS or VSS for vSphere traffic
- 1 N-VDS for NSX-T traffic (NSX-T transport nodes)

TKGI v1.8 does not support Converged VDS (C-VDS) that is newly available on vSphere 7.0. The C-VDS let you run vSphere traffic and NSX-T traffic on the same VDS. As a result, TKGI v1.8 does not support VCF 4.x platforms which require C-VDS. A C-VDS is implemented by creating a VDS in vSphere 7 and then using this VDS to configure the NSX-T transport nodes, as opposed to creating an N-VDS.

###<a id='nsxt30-esxi-pgs'></a> Create vSwitch Port-Groups for Edge Nodes

Create vSwitch Port-Groups for the Edge Nodes on the ESXi hosts in the MANAGEMENT-cluster.

For each ESXi host in the MANAGEMENT-cluster, create the following vSwitch Port Groups:
- EDGE-VTEP-PG: VLAN 3127
- EDGE-UPLINK-PG: VLAN trunk (All (4095))

1. Log in to the vCenter Server.

1. Select the ESXi host in the MANAGEMENT-cluster.

1. Select Configure > Virtual switches. 

1. Select Add Networking (upper right).

1. Select the option **Virtual Machine Port Group for a Standard Switch** and click Next.                

1. Select the existing standard switch named `vSwitch0` and click Next.

1. Enter a Network Label, such as `EDGE-VTEP-PG`.

1. Enter a VLAN ID, such as `3127`.

1. Click Finish.

1. Verify that you see the newly created port group.

1. Select Add Networking (upper right).

1. Select the option **Virtual Machine Port Group for a Standard Switch** and click Next.                

1. Select the existing standard switch named `vSwitch0` and click Next.

1. Enter a Network Label, such as `EDGE-UPLINK-PG`.

1. For the VLAN ID, select `All (4095)` from the drop-down.

1. Click Finish.

1. Verify that you see the newly created port group.

###<a id='nsxt30-esxi-mtu'></a> Set vSwitch0 with MTU at 9000
 
For each ESXi host in the MANAGEMENT-cluster, or each ESXi host in the vCenter cluster if you have not created separate Management and Compute clusters, you must enable the virtual switch with jumbo MTU, that is, set vSwitch0 with MTU=9000. If you do not do this, network overlay traffic will jam. The TEP interface for the NSX-T Edge Nodes must be connected to a port group that supports > 1600 bytes. The default is 1500.

1. Select the Virtual Switch on each ESXi host in the MANAGEMENT-cluster, or each host in the vCenter cluster.
 
1. Click Edit. 

1. For the MTU (bytes) setting, enter `9000`.

1. Click OK to complete the operation.

##<a id='nsxt30-edge-nodes'></a> Deploy NSX-T Edge Nodes

In this section you deploy two NSX-T Edge Nodes. 

NSX Edge Nodes provide the bridge between the virtual network environment implemented using NSX-T and the physical network. Edge Nodes for <%= vars.product_short %> run load balancers for <%= vars.control_plane %> traffic, Kubernetes load balancer services, and ingress controllers. See [Load Balancers in <%= vars.product_short %>](./about-lb.html) for more information.

In NSX-T, a load balancer is deployed on the Edge Nodes as a virtual server. The following virtual servers are required for <%= vars.product_short %>:

- 1 TCP Layer 4 virtual server for each Kubernetes service of type:`LoadBalancer`
- 2 Layer 7 global virtual servers for Kubernetes pod ingress resources (HTTP and HTTPS)
- 1 global virtual server for the <%= vars.control_plane %>

The number of virtual servers that can be run depends on the size of the load balancer which depends on the size of the Edge Node. <%= vars.product_short %> supports the `medium` and `large` VM Edge Node form factor, as well as the bare metal Edge Node. The default size of the load balancer deployed by NSX-T for a Kubernetes cluster is `small`. The size of the load balancer can be customized using <a href="./network-profiles-define.html">Network Profiles</a>. 

For this insallation, we use the Large VM form factor for the Edge Node. See [VMware Configuration Maximums](https://configmax.vmware.com/guest?vmwareproduct=VMware%20NSX-T&release=NSX-T%20Data%20Center%203.0.0&categories=17-0) for more information.

###<a id='nsxt30-edge-node-1'></a> Install Edge Node 1 

Deploy the Edge Node 1 VM using the NSX-T Manager interface.

1. From your browser, log in with admin privileges to NSX Manager at `https://NSX-MANAGER-IP-ADDRESS`.

1. In NSX Manager, go to **System** > **Fabric** > **Nodes** > **Edge Transport Nodes**.
  <img src="images/nsxt/nsxt-30/edge-node-01.png">

1. Click **Add Edge VM**.

1. Configure the Edge VM as follows:
  - **Name**: `edge-node-1`
  - **Host name/FQDN**: `edge-node-1.lab.com`
  - **Form Factor**: **Large**
  <img src="images/nsxt/nsxt-30/edge-node-02.png">

1. Configure **Credentials** as follows:
  - **CLI User Name**: `admin`
  - **CLI Password**: Enter a strong password for the `admin` user that complies with the NSX-T requirements.
  - **Enable SSH Login**: Yes
  - **System Root Password**: Enter a strong password for the `root` user that complies with the NSX-T requirements.
  - **Enable Root SSH Login**: Yes
  - **Audit Credentials**: Enter an `audit` user name and password.
  <img src="images/nsxt/nsxt-30/edge-node-03.png">

1. Configure the deployment as follows:
  - **Compute Manager**: vCenter
  - **Cluster**: MANAGEMENT-Cluster
  - **Datastore**: Select the datastore
  <img src="images/nsxt/nsxt-30/edge-node-04.png">

1. Configure the node settings as follows:
  - **IP Assignment**: Static
  - **Management IP**: 10.173.62.49/24, for example
  - **Default Gateway**: 10.173.62.253, for example
  - **Management Interface**: PG-MGMT-VLAN-1548, for example
  <img src="images/nsxt/nsxt-30/edge-node-05.png">

1. Configure the first NSX switch for the Edge Node as follows:
  - **Edge Switch Name**: `switch-overlay` (be sure to use the exact switch name that was configured for `tz-overlay`)
  - **Transport Zone**: `tz-overlay`
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG
  <img src="images/nsxt/nsxt-30/edge-node-06.png">

1. Configure the second NSX switch for the Edge Node as follows:
  - Click **Add Switch** (at the top of the dialog)
  - **Edge Switch Name**: `switch-vlan` (be sure to use the same switch name that was configured for `tz-vlan`)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **Uplinks**: uplink-1 / EDGE-UPLINK-PG
  <img src="images/nsxt/nsxt-30/edge-node-07.png">

1. Click **Finish** to complete the configuration. The installation begins.

1. In vCenter, use the **Recent Tasks** panel at the bottom of the page to verify that you see the Edge Node 1 VM being deployed.

1. Once the process completes, you should see the Edge Node 1 deployed successfully in NSX-T Manager.
  <img src="images/nsxt/nsxt-30/edge-node-08.png">

1. Click the N-VDS link and verify that you see both switches.
  <img src="images/nsxt/nsxt-30/edge-node-09.png">

1. In vCenter verify that the Edge Node is created.
  <img src="images/nsxt/nsxt-30/edge-node-10.png">

###<a id='nsxt30-edge-node-2'></a> Install Edge Node 2

Repeat the same operation for Edge Node 2, and for each additional NSX Edge Node pair you intend to use for <%= vars.product_short %>. 

1. Install `nsx-edge-2` following the same procedure as `nsx-edge-1`.
  - name: `edge-node-2`
  - hostname/FQDN: `edge-node-2.lab.com`, for example
  - Form Factor: Large
  - IP Assignment: Static
  - IP: 10.173.62.58/24, for example
  - GW: 10.173.62.253, , for example
  - Management Interface: `PG-MGMT-VLAN-1548`
  - Edge Switch 1:
    - Name: `switch-overlay` (use the same switch name that was configured for `tz-overlay`)
    - Transport Zone: `tz-overlay`
    - Uplink Profile: `nsx-edge-single-nic-uplink-profile`
    - IP Assignment: `Use IP Pool`
    - IP Pool: `TEP-IP-POOL`
    - Uplinks: `uplink-1` / `EDGE-VTEP-PG`
  - Edge Switch 2:
    - Name: `switch-vlan` (use the same switch name that was configured for tz-vlan)
    - Transport Zone: `tz-vlan`
    - Uplink Profile: `nsx-edge-single-nic-uplink-profile`
    - Uplinks: `uplink-1` / `EDGE-UPLINK-PG`

1. Once done, you should be able to see both Edge Nodes in NSX Manager.
  <img src="images/nsxt/nsxt-30/edge-node-11.png">

##<a id='nsxt30-uplink-profile'></a> Create Uplink Profile for ESXi Transport Node

To configure the TEP, we used the default profile named 'nsx-default-uplink-hostswitch-profile'. However, because the TEP is on VLAN 3127, you must modify the uplink profile for the ESXI Transport Node (TN). NSX-T does not allow you to edit settings the default uplink profile, so we create a new one.

1. Go to system > Fabric > Profiles > Uplink Profiles.
  <img src="images/nsxt/nsxt-30/uplink-profile-01.png">
 
1. Click Add.

1. Configure the Uplink Profiles as follows:
  - Name: `nsx-esxi-uplink-hostswitch-profile`
  - Teamings:
    - Failover order 
    - Active Uplinks: uplink-1
  - Transport VLAN: 3127
  <img src="images/nsxt/nsxt-30/uplink-profile-02.png">

1. Click Add.

1. Verify that the Uplink Profile is created.
  <img src="images/nsxt/nsxt-30/uplink-profile-03.png">

##<a id='nsxt30-esxi-tn'></a> Deploy ESXi Host Transport Nodes Using N-VDS

Deploy each ESXi host in the COMPUTE-cluster as an ESXi host transport node (TN) in NSX-T. If you have not created a separate COMPUTE-cluster for ESXi hosts, deploy each ESXi host in the vSphere cluster as a host transport node in NSX-T.

1. Go to System > Fabric > Nodes > Host Transport Nodes.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-01.png">

1. Expand the Compute Manager and select the ESXi host in the COMPUTE-cluster, or each ESXi host in the vSphere cluster.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-02.png">
 
1. Click Configure NSX.

1. In the Host Details tab, enter a name, such as `10.172.210.57`.

1. In the Configure NSX tab, configure the transport node as follows:
  - Type: `N-VDS` (do not select the VDS option; Converged VDS is not supported by TKGI)
  - Name: `switch-overlay` (you must use the same switch name that was configured for `tz-overlay` transport zone)
  - Transport Zone: `tz-overlay`
  - NIOC profile: `nsx-default-nioc-hostswitch-profile`
  - Uplink Profile: `nsx-esxi-uplink-hostswitch-profile`
  - LLDP profile: `LLDP [Send Packet Disabled]`
  - IP Assignment: `Use IP Pool`
  - IP Pool: `TEP-IP-POOL`
  - Teaming Policy Swtich Mapping
    - Uplinks: `uplink-1`
    - Physical NICs: `vmnic1`
  <img src="images/nsxt/nsxt-30/esxi-host-tn-03.png">

1. Click Finish.

1. Verify that the host TN is configured.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-04.png">

##<a id='nsxt30-check-tep'></a> Verify TEP to TEP Connectivity

To avoid any overlay communication in the future due to MTU issue, test TEP to TEP connectivity and verify that it is working.

1. SSH to edge-node-1 and get the local TEP IP address, such as `192.23.213.1`. Use the command `get vteps` to get the IP.

1. SSH to edge-node-2 and get the local TEP IP address, ushc as `192.23.213.2`. Use the command `get vteps` to get the IP.

1. SSH to the ESXi host and get the TEP IP address, such as `192.23.213.3`. Use the command `esxcfg-vmknic -l` to get the IP. The interface will be `vmk10` and the NetStack will be `vxlan`.

1. From each ESXi transport node, test the connecion to each NSX-T Edge Node, for example:

    # vmkping ++netstack=vxlan 192.23.213.1 -d -s 1572 -I vmk10: OK
    # vmkping ++netstack=vxlan 192.23.213.2 -d -s 1572 -I vmk10: OK

1. From NSX-T edge node 1 and edge node 2 to ESXi TN:
    > vrf 0
    > ping 192.23.213.1 size 1572 dfbit enable: OK

1. From NSX-T edge node 1 to NSX-T edge node 2:
    > vrf 0
    > ping 192.23.213.2 size 1572 dfbit enable: OK

##<a id='nsxt30-edge-cluster'></a> Create NSX-T Edge Cluster

1. Go to system > Fabric > Nodes > Edge Clusters.
  <img src="images/nsxt/nsxt-30/edge-cluster-01.png">
 
1. Click Add.
  - Enter a name, such as `edge-cluster-1`.
  - Add members, including `edge-node-1` and `edge-node-2`.
  <img src="images/nsxt/nsxt-30/edge-cluster-02.png">

1. Click Add.

1. Verify.
  <img src="images/nsxt/nsxt-30/edge-cluster-03.png">

##<a id='nsxt30-uplink-ls'></a> Create Uplink Logical Switch

Create an uplink Logical Switch to be used for the Tier-0 Router.

1. Make sure the "Manager" tab is selected.

1. Go to Networking > Logical Switches.
  <img src="images/nsxt/nsxt-30/uplink-ls-01.png">

1. Click Add.

1. Configure the new logical switch as follows: 
  - Name: `LS-T0-uplink`
  - Transport Zone: `tz-vlan`
  - VLAN: `1548`
  <img src="images/nsxt/nsxt-30/uplink-ls-02.png">

1. Click Add.

1. Verify.
  <img src="images/nsxt/nsxt-30/uplink-ls-03.png">

##<a id='nsxt30-t0-router'></a> Create Tier-0 Router

1. Select Networking from the "Manager" tab.
  <img src="images/nsxt/nsxt-30/tier-0-01.png">
 
1. Select Tier-0 Logical Router.
  <img src="images/nsxt/nsxt-30/tier-0-02.png">
 
1. Click Add.

1. Configure the new Tier-0 Router as follows:
  - Name: `T0-router`
  - Edge Cluster: `edge-cluster-1`
  - HA mode: `Active-Standby`
  - Failover mode: `Non-Preemptive`
  <img src="images/nsxt/nsxt-30/tier-0-03.png">

1. Click Save.
  <img src="images/nsxt/nsxt-30/tier-0-04.png">
 
1. Select the T0 router.

1. Select Configuration > Router ports.
 
1. Click Add.

1. Conigure the T0 router port as follows: 
  - Name: T0-uplink-1
  - Type: uplink
  - Transport Node: edge-node-1
  - Logical Switch: LS-T0-uplink
  - Logical Switch Port: Attach to a new switch port
  - Subnet: 10.173.62.50 / 24

Click on Add
You will see: 
 

Repeat the same operation for the second uplink (connected to edge-node-2 and IP is 10.173.62.51 / 24)

Once completed, you should have:





