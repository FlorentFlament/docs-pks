---
title: Preparing NSX-T for Enterprise PKS
owner: PKS
---

<strong><%= modified_date %></strong>

Before you install <%= vars.product_full %> on vSphere with NSX-T integration, you must prepare your NSX-T environment. Complete all of the steps listed in the order presented to manually create the NSX-T environment for <%= vars.product_short %>.

##<a id='plan'></a> Step 1: Plan Network Topology, Subnets, and IP Blocks

###<a id='plan-topology'></a>Plan NSX-T Deployment Topology

Review [vSphere with NSX-T Version Requirements](vsphere-nsxt-requirements.html)
and [Hardware Requirements for <%= vars.product_short %> on vSphere with NSX-T](vsphere-nsxt-rpd-mpd.html).

Review the [Deployment Toplogies](nsxt-topologies.html) for <%= vars.product_short %> on vSphere with NSX-T, and the [NSX-T Data Center documentation](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/com.vmware.nsxt.install.doc/GUID-10B1A61D-4DF2-481E-A93E-C694726393F9.html) to ensure that your chosen network topology will enable the following communications:

* vCenter, NSX-T components, and ESXi hosts must be able to communicate with each other.
* The BOSH Director VM must be able to communicate with vCenter and the NSX Manager.
* The BOSH Director VM must be able to communicate with all nodes in all Kubernetes clusters.
* Each <%= vars.product_short %>-provisioned Kubernetes cluster deploys the NSX-T Node Agent and the Kube Proxy that run as BOSH-managed processes on each worker node.

In addition, the NSX-T Container Plugin (NCP) runs as a BOSH-managed process on the Kubernetes master node. In a multi-master <%= vars.product_short %> deployment, the NCP process runs on all master nodes. However, the process is active only on one master node. If the NCP process on an active master is unresponsive, BOSH activates another NCP process. Refer to the [NCP documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) for more information.

###<a id='plan-cidrs'></a>Plan Network CIDRs

Before you install <%= vars.product_short %> on vSphere with NSX-T, you should plan for the CIDRs and IP blocks that you are using in your deployment.

Plan for the following network CIDRs in the IPv4 address space according to the instructions in the VMware [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html).

* **VTEP CIDRs**: One or more of these networks host your GENEVE Tunnel Endpoints on your NSX Transport Nodes. Size the networks to support all of your expected Host and Edge Transport Nodes. For example, a CIDR of `192.168.1.0/24` provides 254 usable IPs.

* **PKS MANAGEMENT CIDR**: This small network is used to access <%= vars.product_short %> management components such as Ops Manager, BOSH Director, the PKS Service VM, and the Harbor Registry VM (if deployed). For example, a CIDR of `10.172.1.0/28` provides 14 usable IPs. For the [No-NAT deployment topologies](nsxt-topologies.html#topology-no-nat-virtual-switch), this is a corporate routable subnet /28. For the [NAT deployment topology](nsxt-topologies.html#topology-nat), this is a non-routable subnet /28, and DNAT needs to be configured in NSX-T to access the <%= vars.product_short %> management components.

* **PKS LB CIDR**: This network provides your load balancing address space for each Kubernetes cluster created by <%= vars.product_short %>. The network also provides IP addresses for Kubernetes API access and Kubernetes exposed services. For example, `10.172.2.0/24` provides 256 usable IPs. This network is used when creating the `ip-pool-vips` described in [Creating NSX-T Objects for <%= vars.product_short %>](nsxt-create-objects.html), or when the services are deployed. You enter this network in the
**Floating IP Pool ID** field in the **Networking** pane of the <%= vars.product_tile %> tile.

###<a id='plan-ip-blocks'></a>Plan IP Blocks

When you install <%= vars.product_short %> on NSX-T, you are required to specify the **Pods IP Block ID** and **Nodes IP Block ID** in the **Networking** pane of the <%= vars.product_tile %> tile. These IDs map to the two IP blocks you must configure in NSX-T: the Pods IP Block for Kubernetes pods, and the Node IP Block for Kubernetes nodes (VMs). For more information, see the [Networking](installing-nsx-t.html#networking) section of _Installing <%= vars.product_short %> on vSphere with NSX-T Integration_.

  <img src="images/nsxt/nsxt-ip-blocks.png" alt="Required IP Blocks for NSX-T">

####<a id='pods-ip-block'></a>Pods IP Block

Each time a Kubernetes namespace is created, a subnet from the **Pods IP Block** is allocated. The subnet size carved out from this block is /24, which means a maximum of 256 pods can be created per namespace. When a Kubernetes cluster is deployed by <%= vars.product_short %>, by default 3 namespaces are created. Often additional namespaces will be created by operators to facilitate cluster use. As a result, when creating the **Pods IP Block**, you must use a CIDR range larger than /24 to ensure that NSX has enough IP addresses to allocate for all pods. The recommended size is /16. For more information, see [Creating NSX-T Objects for <%= vars.product_short %>](nsxt-create-objects.html).

  <img src="images/nsxt/pods-ip-block.png" alt="Pods IP Block">

<p class="note"><strong>Note</strong>: By default, <strong>Pods IP Block</strong> is a block of non-routable, private IP addresses.
After you deploy <%= vars.product_short %>, you can define a network profile that specifies a routable IP block for your pods.
The routable IP block overrides the default non-routable <strong>Pods IP Block</strong> when a Kubernetes cluster is deployed using that network profile. For more information, see <a href="network-profiles.html#routable-pods">Routable Pods</a> in <em>Using Network Profiles (NSX-T Only)</em>.</p>

####<a id='nodes-ip-block'></a>Nodes IP Block

Each Kubernetes cluster deployed by <%= vars.product_short %> owns a /24 subnet. To deploy multiple Kubernetes clusters, set the **Nodes IP Block ID** in the **Networking** pane of the <%= vars.product_tile %> tile to larger than /24. The recommended size is /16. For more information, see [Creating NSX-T Objects for <%= vars.product_short %>](nsxt-create-objects.html).

  <img src="images/nsxt/nodes-ip-block.png" alt="Nodes IP Block">

<p class="note"><strong>Note</strong>: You can use a smaller nodes block size for no-NAT environments with a limited number of routable subnets.
For example, /20 allows up to 16 Kubernetes clusters to be created.</p>

###<a id='reserved-ip-blocks'></a>Reserved IP Blocks

The <%= vars.product_short %> Management Plane must not use the use 172.17.0.0/16 subnet. This restriction applies to all virtual machines (VMs) deployed during the <%= vars.product_short %> installation process, including the PKS control plane, Ops Manager, BOSH Director, and Harbor Registry.

In addition, do not use any of the IP blocks listed below for Kubernetes master or worker node VMs, or for Kubernetes pods.
If you create Kubernetes clusters with any of the blocks listed below, the Kubernetes worker nodes cannot reach Harbor or internal Kubernetes services.

The Docker daemon on the Kubernetes worker node uses the subnet in the following CIDR range.
Do not use IP addresses in the following CIDR range:

  * 172.17.0.1/16
  * 172.18.0.1/16
  * 172.19.0.1/16
  * 172.20.0.1/16
  * 172.21.0.1/16
  * 172.22.0.1/16

If <%= vars.product_short %> is deployed with Harbor, Harbor uses the following CIDR ranges for its internal Docker bridges.
Do not use IP addresses in the following CIDR range:

  * 172.18.0.0/16
  * 172.19.0.0/16
  * 172.20.0.0/16
  * 172.21.0.0/16
  * 172.22.0.0/16

Each Kubernetes cluster uses the following subnet for Kubernetes services.
Do not use the following IP block for the Nodes IP Block:

  * 10.100.200.0/24

##<a id='nsx-manager'></a> Step 2: Install and Configure NSX-T Data Center for Enterprise PKS

See <a href="nsxt-install-config-steps.html">Installing and Configuring NSX-T for Enterprise PKS</a>.

##<a id='nsx-esxi'></a> Step 3: Create NSX-T Objects for <%= vars.product_short %> Management Plane

Prepare the vSphere and NSX-T infrastructure for the <%= vars.product_short %> Management Plane where the PKS, Ops Manager, BOSH Director, and Harbor Registry VMs are deployed. This includes a vSphere resource pool for <%= vars.product_short %> management components, an NSX Tier-1 (T1) Logical Switch, and an NSX Tier-1 Logical Router and Port. For instructions, see [Prepare <%= vars.product_short %> Management Plane](nsxt-prepare-mgmt-plane.html).

If you are using the <a href="nsxt-topologies.html#topology-nat">NAT Topology</a>, create the following NAT rules on the T0 Router. For instructions, see [Prepare Management Plane](nsxt-prepare-mgmt-plane.html).

<table>
  <tr>
    <th>Type</th>
    <th>For</th>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External > Ops Manager</td>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External > Harbor (optional)</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > vCenter and NSX-T Manager</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > DNS</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > NTP</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > LDAP/AD (optional)</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > ESXi</td>
  </tr>
</table>

##<a id='nsx-comp-plane'></a> Step 4: Create NSX-T Objects for <%= vars.product_short %> Compute Plane

Create Resource Pools for AZ-1 and AZ-2, which map to the Availability Zones you will create when you configure BOSH Director and reference when you install the <%= vars.product_tile %> tile. In addition, create SNAT rules on the T0 router:

- One for K8s Master Nodes (hosting NCP) to reach the NSX-T Manager
- One for Kubernetes Master Node Access to LDAP/AD (optional)

For instructions, see [Prepare Compute Plane](nsxt-prepare-compute-plane.html).

##<a id='nsx-om'></a> Step 5: Deploy Ops Manager in the NSX-T Environment

Deploy a supported version of Ops Manager on the NSX-T Management Plane network.
For instructions, see <a href="vsphere-nsxt-om-deploy.html">Deploy Ops Manager on vSphere with NSX-T</a>.

##<a id='nsx-mgr-cert'></a> Step 6: Generate NSX Manager Certificate

Generate the CA Cert for the NSX Manager and import the certificate to NSX Manager.
For instructions, see <a href="generate-nsx-ca-cert.html">Generate the NSX Manager CA Cert</a>.

##<a id='nsx-bosh'></a> Step 7: Configure BOSH Director for vSphere with NSX-T

Create BOSH availability zones (AZs) that map to the Management and Compute resource pools in vSphere, and the Management and Control plane networks in NSX-T.
For instructions, see <a href="vsphere-nsxt-om-config.html">Configure BOSH Director for vSphere with NSX-T</a>.

##<a id='nsx-pi-cert'></a> Step 8: Generate NSX Manager Principal Identity Certificate

Generate the NSX Manager Super User Principal Identity Certificate and register it with the NSX Manager using the NSX API.
For instructions, see <a href="generate-nsx-pi-cert.html">Generate the NSX Manager PI Cert</a>.

##<a id='nsx-pks-obj'></a> Step 9: Create NSX-T Objects for <%= vars.product_short %>

Create IP blocks for the [node networks](#nodes-ip-block) and the [pod networks](#pods-ip-block). The subnets for both nodes and pods should have a size of 256 (/16). See [Plan IP Blocks](#plan-ip-blocks) and [Reserved IP Blocks](#reserved-ip-blocks) for details.

In addition, create a Floating IP Pool from which to assign routable IP addresses to components. This network provides your load balancing address space for each Kubernetes cluster created by <%= vars.product_short %>. The network also provides IP addresses for Kubernetes API access and Kubernetes exposed services.

These network objects are required to configure the <%= vars.product_tile %> tile for NSX-T networking. For instructions, see <a href="./nsxt-create-objects.html">Create NSXT Object for <%= vars.product_short %></a>.

##<a id='nsx-pks-install'></a> Step 10: Install <%= vars.product_short %> on vSphere with NSX-T

At this point your NSX-T environment is prepared for <%= vars.product_short %> installation using the <%= vars.product_tile %> tile in Ops Manager. For instructions, see <a href="./installing-nsx-t.html">Installing <%= vars.product_short %> on vSphere with NSX-T</a>.

##<a id='nsx-pks-harbor'></a> Step 11: Install Harbor Harbor Registry for <%= vars.product_short %>

The VMware Harbor Registry is recommended for <%= vars.product_short %>. Install Harbor in the NSX Management Plane with other <%= vars.product_short %> components (PKS API, Ops Manager, and BOSH). For instructions, see <a href="https://docs.pivotal.io/partners/vmware-harbor/integrating-pks.html">Installing Harbor Registry on vSphere with NSX-T</a> in the VMware Harbor documentation.

If you are using the [NAT deployment topology](nsxt-topologies.html#topology-nat) for <%= vars.product_short %>, create a DNAT rule that maps the private Harbor IP address to a routable IP address from the floating IP pool on the PKS management network. See <a href="https://docs.pivotal.io/partners/vmware-harbor/integrating-pks.html#create-dnat">Create DNAT Rule</a>.

##<a id='nsx-pks-adv'></a> Step 12: Create Network Profiles to Customize NSX-T Networking

Once <%= vars.product_short %> is installed, you may want to <a href="./network-profiles-define.html">Define Network Profiles</a> to customize NSX-T networking objects, such as load balancer size, custom Pods IP Block, routable Pods IP Block, configurable CIDR range for the Pods IP Block, custom Floating IP block, and more.