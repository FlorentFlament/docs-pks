---
title: Installing and Configuring NSX-T v2.4.x for Enterprise PKS
owner: PKS-NSXT
---

<strong><%= modified_date %></strong>

This topic provides instructions for installing NSX-T v2.4 for use with <%= vars.product_full %> on vSphere.

##<a id='nsxt-24-prereqs'></a> Prerequisites

Before you begin this procedure, ensure that you have successfully completed all preceding steps for installing <%= vars.product_short %> on vSphere with NSX-T, including:

<ul>
  <li>
    <a href="./vsphere-nsxt-requirements.html">vSphere with NSX-T Version Requirements</a>
  </li>
  <li>
    <a href="./vsphere-nsxt-rpd-mpd.html">Hardware Requirements for <%= vars.product_short %> on vSphere with NSX-T</a>
  </li>
  <li>
    <a href="./nsxt-topologies.html">NSX-T Deployment Topologies for <%= vars.product_short %></a>
  </li>
  <li>
    <a href="./nsxt-prepare-env.html">Preparing to Deploy <%= vars.product_short %> with NSX-T on vSphere</a>
  </li>
  <li>
    <a href="./nsxt-deploy-24.html">Considerations for Deploying NSX-T v2.4 for <%= vars.product_short %> on vSphere</a>
  </li>
</ul>

##<a id='nsxt-24-install'></a> Installing NSX-T v2.4

To perform a new installation of NSX-T v2.4 for Enterprise PKS v1.4, complete the following steps.

###<a id='nsx-install-prepare'></a> Step 1: Prepare for Installing NSX-T v2.4

[Prepare for Installing NSX-T v2.4](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-EC9633E5-3CF1-433B-B7B5-FC9BBB3EE8F0.html).

###<a id='nsx-install-mgmt-cluster'></a> Step 2: Deploy NSX-T Manager Appliances and Configure the NSX-T Management Cluster

1. [Install the NSX Manager Unified Appliance](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-FA0ABBBD-34D8-4DA9-882D-085E7E0D269E.html) using the OVA file, for example: `nsx-unified-appliance-2.4.1.0.0.13716579.ova`.

1. [Log In to the Newly Created NSX Manager](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-BF9FF9E2-47BD-466F-BDD2-8FF5145412E5.html).

1. [Add a Compute Manager](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-D225CAFC-04D4-44A7-9A09-7C365AAFCA0E.html).

1. [Deploy Two Additional NSX Manager Nodes to Form a NSX Management Cluster](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-B89F5831-62E4-4841-BFE2-3F06542D5BF5.html) Using the NSX Manager UI.

1. [Configure a Virtual IP Address and Cluster Certificate for the NSX v2.4 Management Cluster](./nsxt-mgmt-vip.html). Alternatively, [Provision a Load Balancer for the NSX-T v2.4 Management Cluster](./nsxt-mgmt-lb.html).
	
<p class="note"><strong>Note:</strong> If you do not require scalability, configure a Cluster VIP to achieve HA for the NSX-T Management Cluster. If you need to scale, provision a load balancer for the NSX-T Management Cluster.</p>

<p class="note"><strong>Note:</strong> To provision a load balancer, you will first need to deploy and configure NSX-T Edge Nodes.</p>

###<a id='nsx-install-edges'></a> Step 3: Deploy NSX Edge Nodes and Join Them with the Management Plane

NSX-T Edge Nodes run load balancers for <%= vars.product_short %> API traffic, load balancer services for Kubenetes pods, and ingress controllers for Kubernetes pods.

<%= vars.product_short %> supports active/standby Edge Node failover and requires at least two Edge Nodes. In addition, <%= vars.product_short %> requires the Edge Node Large VM (8 vCPU, 16 GB of RAM, and 120 GB of storage). The default size of the LB provisioned for <%= vars.product_short %> is small. You can customize this after deploying <%= vars.product_short %> using <a href="network-profiles.html">Network Profiles</a>.

1. [Install One or More Pair of NSX Edge Nodes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-AECC66D0-C968-4EF2-9CAD-7772B0245BF6.html) using the OVA file, for example: `nsx-edge-2.4.1.0.0.13716583.ova`.

<p class="note"><strong>Warning:</strong> For Enterprise PKS you must install a large size VM form factor or the bare metal Edge Node.</p>

1. [Join Each NSX Edge Node with the Management Plane](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-11BB4CF9-BC1D-4A76-A32A-AD4C98CBF25B.html).

The table below lists the maximum number of load balancers per Edge Node form factor.

Edge Node Type  | LB Small Max  | LB Medium Max | LB Large Max  | Supported by <%= vars.product_short %>
----------------|---------------|---------------|---------------|-----------------
Edge VM Small   | 0             | 0             | 0        		| No
Edge VM Medium  | 1             | 0             | 0        		| No
Edge VM Large   | 40            | 4             | 0        		| Yes
Edge Bare Metal | 750           | 100           | 7        		| Yes

Keep in mind the following requirements for NSX Edge Nodes with <%= vars.product_short %>:

* <%= vars.product_short %> requires the NSX-T Edge Node large VM (8 vCPU and 16 GB of RAM) or the bare metal Edge Node. For more information, see <a href="./vsphere-nsxt-rpd-mpd.html">Hardware requirements for <%= vars.product_short %> on vSphere with NSX-T</a>.
* The default load balancer deployed by NSX-T for an <%= vars.product_short %>-provisioned Kubernetes cluster is the small load balancer. The size of the load balancer can be customized using <a href="./network-profiles-define.html">Network Profiles</a>.
* Edge Node VMs can only be deployed on Intel-based ESXi hosts.
* The large load balancer requires a bare metal Edge Node.
* For high-availability Edge Nodes are deployed as pairs within an Edge Cluster. The minimum number of Edge Nodes per Edge Cluster is 2; the maximum is 10. <%= vars.product_short %> supports active/standby mode only. In standby mode, the standby LB is not available for use while the active LB is active. To determine the maximum number of load balancers per Edge Cluster, multiply the maximum number of LBs for the Edge Node type by the number of Edge Nodes and divide by 2. For example, with 10 Edge VM Large nodes in an Edge Cluster, you can have up to 200 small LB instances (40 x 10 / 2), or up to 20 medium LB instances (4 x 10 / 2).
* <%= vars.product_short %> deploys a virtual server for each load balancer instance. For service of type load balancer, it is one virtual server per service. There are two global virtual servers deployed for ingress resources (HTTP and HTTPS). And there is one global virtual server for the PKS API. For more information, see <a href="network-profiles-define.html">Defining Network Profiles</a>.

###<a id='nsx-install-vib'></a> Step 4: Enable VIB Repository Service

The VIB repository service provides access to native libraries for NSX Transport Nodes. VIB must be enabled before you proceed further with deploying NSX. 

For instructions, see [Enable Repository Service on NSX Manager](./nsxt-deploy.html#enable-repo).

<p class="note"><strong>NOTE:</strong> You must repeat this step for each NSX Manager VM.</p>

###<a id='nsx-install-tep'></a> Step 5: Create TEP IP Pool

Create Tunnel Endpoint IP Pool (TEP IP Pool) within the usable range of the **VTEP CIDR** that you defined in preparation for installing NSX-T. The TEP IP Pool is used for NSX Transport Nodes. 

For instructions, see [Create an IP Pool for Tunnel Endpoint IP Addresses](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-E7F7322D-D09B-481A-BD56-F1270D7C9692.html).

###<a id='nsx-instally-tzs'></a> Step 6: Create Overlay and VLAN Transport Zones

Create two [Transport Zones](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-7EA5F174-9D29-45DF-BDE8-94EAE57F9B62.html):

- A Overlay Transport Zone (named `TZ-Overlay`, for example) for use with <%= vars.product_short %> Control Plane services and Kubernetes Cluster deployment overlay networks. 
- A VLAN Transport Zone (named `TZ-VLAN`, for example) for NSX Edge uplinks (ingress/egress) for <%= vars.product_short %>-managed Kubernetes clusters. 

For instructions, see [Create Overlay and VLAN Transport Zones](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html).

###<a id='nsx-install-uplink'></a> Step 7: Create an Uplink Profile for Edge Nodes

Create an NSX-T Uplink Profile for NSX Edge Nodes to be used with <%= vars.product_short %>. 

For instructions, see [Create an Uplink Profile](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-50FDFDFB-F660-4269-9503-39AE2BBA95B4.html).

###<a id='nsx-install-tns'></a> Step 8: Create Transport Edge Nodes

Create NSX Edge Transport Nodes to allow Edge Nodes to exchange traffic for virtual networks among other NSX nodes. 

For instructions, see [Create Edge Transport Nodes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-53295329-F02F-44D7-A6E0-2E3A9FAE6CF9.html).

###<a id='nsx-install-edge-cluster'></a> Step 9: Configure an Edge Cluster

Configure an [NSX Edge Cluster](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-898099FC-4ED2-4553-809D-B81B494B67E7.html) and add each NSX Edge Transport Node to the Edge Cluster. 

For instructions, see [Create an NSX Edge Cluster](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/installation/GUID-898099FC-4ED2-4553-809D-B81B494B67E7.html).

###<a id='nsx-install-edge-ha'></a> Step 10: Configure NSX Edge Nodes for HA

Configure NSX Edge Nodes for high availability (HA) using Active/Standby mode to support failover, as shown in the following figure. 

![NSX Edge High Availability](images/vsphere/nsxt-edge-ha.png)

For instructions, see [Configure Edge HA](nsxt-deploy.html#configure-edge-ha).

<p class="note"><strong>Note</strong>: If the T0 Router is not configured for HA as decribed in <a href="nsxt-deploy.html#create-edge-ha">Configure Edge Nodes for HA</a>, failover to the standby Edge Node will not occur.</p>

###<a id='nsx-install-esxi'></a> Step 11: Prepare ESXi Hosts for <%= vars.product_short %> Compute Plane

An [NSX Transport Node](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/com.vmware.nsxt.install.doc/GUID-FCC5390E-3489-47E8-ABE6-2F7FD43775BD.html) allows NSX Nodes to exchange traffic for virtual networks. ESXi hosts dedicated to the <%= vars.product_short %> Compute Cluster must be prepared as transport nodes. For instructions, see [Prepare Compute Cluster ESXi Hosts](nsxt-deploy.html#prepare-esxi).

<p class="note"><strong>Note</strong>: The Transport Nodes must be placed on free host NICs not already used by other vSwitches on the ESXi host. Use the <code>VTEPS</code> IP pool that allows ESXi hosts to route and communicate with each other, as well as other Edge Transport Nodes.</p>

For instructions, see [Prepare ESXi Hosts as Transport Nodes for NSX-T](./nsxt-deploy.html#prepare-esxi).

##<a id='pks-install'></a> Installing Enteprise PKS 1.4 on NSX-T 2.4

To perform a new installation of Enterprise PKS v1.4.1 on NSX-T v2.4, complete the following steps.

###<a id='pks-install-t0'></a> Step 1: Create T0 Logical Router

[NSX Tier-0 Logical Routers](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-3F163DEE-1EE6-4D80-BEBF-8D109FDB577C.html) are used to route data between the NSX-T virtual network and the physical network. 

For instructions, see [Create T0 Router](nsxt-deploy.html#create-t0-router). [Create a Tier-0 Logical Router](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-7891E6E7-606D-4F79-8AB7-BC01898F9FE7.html).

Make sure you use the **Advanced Networking and Security** tab in NSX Manager to create the Tier-0 Router.


##<a id='pks-install-mgmt-plane'></a> Step 2: Create NSX-T Objects for <%= vars.product_short %> Management Plane

Prepare the vSphere and NSX-T infrastructure for the <%= vars.product_short %> Management Plane where the PKS, Ops Manager, BOSH Director, and Harbor Registry VMs are deployed. This includes a vSphere resource pool for <%= vars.product_short %> management components, an NSX [Tier-1 (T1) Logical Switch](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-23194F9A-416A-40EA-B9F7-346B391C3EF8.html), and an NSX [Tier-1 Logical Router and Port](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-DAEF8457-8363-4F33-84DA-68AA36A2DE3C.html). 

For instructions, see [Prepare <%= vars.product_short %> Management Plane](nsxt-prepare-mgmt-plane.html).

If you are using the <a href="nsxt-topologies.html#topology-nat">NAT Topology</a>, create the following NAT rules on the T0 Router. For complete instructions, see [Prepare Management Plane](nsxt-prepare-mgmt-plane.html).

<table>
  <tr>
    <th>Type</th>
    <th>For</th>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External > Ops Manager</td>
  </tr>
  <tr>
    <td>DNAT</td>
    <td>External > Harbor (optional)</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > vCenter and NSX-T Manager</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > DNS</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > NTP</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > LDAP/AD (optional)</td>
  </tr>
  <tr>
    <td>SNAT</td>
    <td>PKS Management Plane > ESXi</td>
  </tr>
</table>


###<a id='pks-install-comp-plane'></a> Step 3: Create NSX-T Objects for <%= vars.product_short %> Compute Plane

Create Resource Pools for AZ-1 and AZ-2, which map to the Availability Zones you will create when you configure BOSH Director and reference when you install the <%= vars.product_tile %> tile. In addition, create SNAT rules on the T0 router:

- One for K8s Master Nodes (hosting NCP) to reach the NSX-T Manager
- One for Kubernetes Master Node Access to LDAP/AD (optional)

For instructions, see [Prepare Compute Plane](nsxt-prepare-compute-plane.html).

###<a id='pks-install-om'></a> Step 4: Deploy Ops Manager in the NSX-T Environment

Deploy a supported version of Ops Manager on the NSX-T Management Plane network.

For instructions, see <a href="vsphere-nsxt-om-deploy.html">Deploy Ops Manager on vSphere with NSX-T</a>.

###<a id='pks-install-mgr-cert'></a> Step 5: Generate NSX Manager Certificate

Generate the CA Cert for the NSX Manager and import the certificate to NSX Manager.
For instructions, see <a href="generate-nsx-ca-cert.html">Generate the NSX Manager CA Cert</a>.

1. [Generate and Register the NSX Manager Cluster Certificate](./generate-nsx-ca-cert-24.html) (if you have not already done so). 

###<a id='pks-install-bosh'></a> Step 6: Configure BOSH Director for vSphere with NSX-T

Create BOSH availability zones (AZs) that map to the Management and Compute resource pools in vSphere, and the Management and Control plane networks in NSX-T.
For instructions, see <a href="vsphere-nsxt-om-config.html">Configure BOSH Director for vSphere with NSX-T</a>.

1. [Configure BOSH Director with NSX-T for Enterprise PKS](./vsphere-nsxt-om-config.html). 

###<a id='pks-install-pi-cert'></a> Step 7: Generate NSX Manager Principal Identity Certificate

Generate the NSX Manager Super User Principal Identity Certificate and register it with the NSX Manager using the NSX API.
For instructions, see <a href="generate-nsx-pi-cert.html">Generate the NSX Manager PI Cert</a>.

1. [Generate and Register the NSX Manager Superuser Principal Identity Certificate and Key](./generate-nsx-pi-cert.html).

###<a id='pks-install-nsxt-objects'></a> Step 8: Create NSX-T Objects for <%= vars.product_short %>

Create IP blocks for the [node networks](#nodes-ip-block) and the [pod networks](#pods-ip-block). The subnets for both nodes and pods should have a size of 256 (/16). See [Plan IP Blocks](#plan-ip-blocks) and [Reserved IP Blocks](#reserved-ip-blocks) for details.

In addition, create a [Floating IP Pool](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/administration/GUID-A27DF20A-5162-40F5-B7D5-2DF8B6AE5DBE.html) from which to assign routable IP addresses to components. This network provides your load balancing address space for each Kubernetes cluster created by <%= vars.product_short %>. The network also provides IP addresses for Kubernetes API access and Kubernetes exposed services.

These network objects are required to configure the <%= vars.product_tile %> tile for NSX-T networking. For instructions, see <a href="./nsxt-create-objects.html">Create NSXT Object for <%= vars.product_short %></a>.

1. [Create NSX-T Objects for Enterprise PKS](nsxt-create-objects.html) using the **System** tab in NSX Manager.

###<a id='pks-install-pks'></a> Step 9: Install <%= vars.product_short %> on vSphere with NSX-T

At this point your NSX-T environment is prepared for <%= vars.product_short %> installation using the <%= vars.product_tile %> tile in Ops Manager. For instructions, see <a href="./installing-nsx-t.html">Installing <%= vars.product_short %> on vSphere with NSX-T</a>.

1. [Install Enterprise PKS on vSphere with NSX-T v2.4](./installing-nsx-t.html).

###<a id='pks-install-harbor'></a> Step 10: Install Harbor Harbor Registry for <%= vars.product_short %>

The VMware Harbor Registry is recommended for <%= vars.product_short %>. Install Harbor in the NSX Management Plane with other <%= vars.product_short %> components (PKS API, Ops Manager, and BOSH). For instructions, see <a href="https://docs.vmware.com/en/VMware-Enterprise-PKS/1.4/vmware-harbor-registry/GUID-index.html">Installing Harbor Registry on vSphere with NSX-T</a> in the VMware Harbor documentation.

If you are using the [NAT deployment topology](nsxt-topologies.html#topology-nat) for <%= vars.product_short %>, create a DNAT rule that maps the private Harbor IP address to a routable IP address from the floating IP pool on the PKS management network. See <a href="https://docs.vmware.com/en/VMware-Enterprise-PKS/1.4/vmware-harbor-registry/GUID-integrating-pks.html">Create DNAT Rule</a>.

###<a id='pks-install-post'></a> Step 26: Perform Post-Installation NSX-T Configurations as Necessary

Once <%= vars.product_short %> is installed, you may want to perform additional NSX-T configurations to support customization of Kubernetes clusters at deployment time, such as:

- <a href="./proxies.html">Configuring an HTTP Proxy</a> to proxy outgoing HTTP/S traffic from NCP, PKS, BOSH, and Ops Manager to vSphere infrastructure components (vCenter, NSX Manager)
- <a href="./network-profiles-define.html">Defining Network Profiles</a> to customize NSX-T networking objects, such as load balancer size, custom Pods IP Block, routable Pods IP Block, configurable CIDR range for the Pods IP Block, custom Floating IP block, and more.
- <a href="./nsxt-multi-t0.html">Configuring Multiple Tier-0 Routers</a> to support customer/tenant isolation
