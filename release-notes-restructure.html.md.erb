---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.4.x.

## <a id='releases'></a> Releases

### <a id="1.4.1"></a>v1.4.1

**Release Date**: June 20, 2019

#### <a id="v1.4.1-components"></a> Component Version Table

<table class="nice">
    <th>Component</th>
    <th>Included Version</th>
    <tr>
        <td>Kubernetes</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.26.0</td>
    </tr>
        <tr>
        <td>uaa</td>
        <td> ? </td>
    </tr>
</table>

#### <a id="v1.4.1-compatibility"></a> Product Compatibility Table

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

<table class="nice">
    <th>Product</th>
    <th>Compatible Versions</th>
    <tr>
        <td>Ops Manager</td>
        <td>v2.4.2+, v2.5.x, v2.6.x</td>
    </tr>
    <tr>
        <td>Ops Manager (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later, v2.6.x</td>
    </tr>
    <tr>
        <td>Stemcell</td>
        <td>v250.25 <br><strong>Note:</strong> To address the Zombieload CVE, upload stemcell v250.48.<br> For more information, see the <a href="https://community.pivotal.io/s/article/mitigating-zombieload-for-pivotal-container-service-pks">Mitigating Zombieload for Pivotal Container Service (PKS)</a> KB article.</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.3.1, v2.4.0.1, v2.4.1 (recommended) </td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">docker-boshrelease</a></td>
    </tr>
    <tr>
        <td>BBR version</td>
        <td>v1.8.0</td>
    </tr>
</table>

#### <a id="v1.4.1-changes"></a> Changes Included in this Release

<%= vars.product_short %> v1.4.1 adds the following:

* **[Feature Improvement]** Support for VMware NSX-T v2.4.1.
For more information, see [VMware NSX-T Data Center 2.4.1 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4.1/rn/VMware-NSX-T-Data-Center-241-Release-Notes.html).
* **[Feature Improvement]** Support for VMware NCP v2.4.1.
For more information, see [NSX Container Plugin 2.4 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/rn/NSX-Container-Plugin-Release-Notes.html) in the VMware documentation.
* **[Feature Improvement]** Support for provisioning an NSX-T load balancer in front of the NSX management cluster.
For more information, see [Provisioning a Load Balancer for the NSX-T v2.4 Management Cluster](./nsxt-mgmt-lb.html).
* **[Feature Improvement]** Ability to tune NCP rate limits.
* Configurable Kubernetes service network CIDR range. For more information, see [Networking](./installing-nsx-t.html#networking)
in _Installing Enterprise PKS on vSphere with NSX-T_.
* **[Feature Improvement]** Configurable node IP block. For more information, see [Defining Network Profiles](./network-profiles-define.html).
* **[Feature Improvement]** Improved functionality for the `/v1beta1/cluster` API endpoint to allow users to list and resize clusters that have compute profiles associated with them.
For more information, see [List Clusters with Compute Profile](./compute-profiles.html#list-clusters) and [Resize a Cluster with a Compute Profile](./compute-profiles.html#resize) in _Using Compute Profile (vSphere Only)_.

### <a id="v1.4.0"></a>v1.4.0

**Release Date**: April 25, 2019

#### <a id="v1.4.0-components"></a> Component Version Table

<table class="nice">
    <th>Component</th>
    <th>Included Version</th>
    <tr>
        <td>Kubernetes</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker</td>
        <td>v0.26.0</td>
    </tr>
        <tr>
        <td>uaa</td>
        <td> ? </td>
    </tr>
</table>

#### <a id="v1.4.1-compatibility"></a> Product Compatibility Table

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

<table class="nice">
    <th>Product</th>
    <th>Compatible Versions</th>
    <tr>
        <td>Ops Manager</td>
        <td>v2.4.2+, v2.5.x</td>
    </tr>
    <tr>
        <td>Ops Manager (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later</td>
    </tr>
    <tr>
        <td>Stemcell</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.3.1, v2.4.0.1*</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>BBR version</td>
        <td>v1.8.0</td>
    </tr>
</table>

\* If you are upgrading to NSX-T 2.4 on PKS v1.4.0, then do the following:

- Use the official VMware NSX-T Data Center 2.4 build.
- Apply the NSX-T v2.4.0.1 hot-patch. For more information, see [KB article 67499](https://kb.vmware.com/s/article/67449) in the VMware Knowledge Base.
- To obtain the NSX-T v2.4.0.1 hot-patch, open a support ticket with VMware Global Support Services (GSS) for NSX-T Engineering.

## <a id="new-features"></a> New Features in PKS v1.4

<%= vars.product_short %> v1.4 adds the following:

* Operators can configure up to ten sets of resource types, or _plans_, in the Enterprise PKS tile. All
plans except the first can made available or unavailable to developers deploying clusters. Plan 1
must be configured and made available as a default for developers. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can deploy up to 5 master nodes per plan. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can install PKS and Pivotal Application Service (PAS) on the same instance of Ops
Manager.

* Improved workflow for managing cluster access. For more information, see [Grant Cluster Access](manage-users.html#cluster-access) in _Managing Users in <%= vars.product_short %> with UAA_.

* Operators can create webhook ClusterSink resources. A webhook ClusterSink resource batches logs into 1 second units, wraps the resulting payload in JSON, and uses the POST method to deliver the logs to the address of your log management service. For more information see, [Create a Webhook ClusterSink Resource with YAML and kubectl](create-sinks.html#webhook-cluster-sink) in _Creating Sinks_.

* Operators can set quotas for maximum memory and CPU utilization in a PKS deployment. For more information, see [Managing Resource Usage](resource-usage.html). This is a beta feature.
    <%= partial 'beta-component' %>

* Operators can enable the PodSecurityPolicy admission plugin on a per-plan basis requiring cluster users to have policy, role, and role binding permissions to deploy pod workloads. See [Pod Security Policies](./pod-security-policy.html) for more information.

* Operators can enable the SecurityContextDeny admission plugin on a per-plan basis to prohibit the use of security context configurations on pods and containers. See [Security Context Deny](./security-context-deny.html) for more information.

* Operators can enable the DenyEscalatingExec admission plugin on a per-plan basis to prohibit the use of certain commands for containers that allow host access. See [Deny Escalating Execution](./deny-escalating-execution.html) for more information.

* Operators using vSphere can use HostGroups to define Availability Zones (AZs) for clusters in BOSH. See [Using vSphere Host Groups](./vsphere-host-group.html).

* Operators using vSphere can configure compute profiles to specify which vSphere
resources are used when deploying Kubernetes clusters in a PKS deployment.
For more information, see [Using Compute Profiles (vSphere Only)](compute-profiles.html).
    <%= partial 'beta-component' %>

* Operators using vSphere with NSX-T can update a Network Profile and add to or reorder the **Pods IP Block** IDs. For more information, see the [Change the Network Profile for a Cluster](./network-profiles.html#update-profile) section of _Using Network Profiles (NSX-T Only)_.

## <a id="v1.4.1-upgrade"></a> How to Upgrade

The following upgrade paths are supported for PKS v1.4.x.

- You can upgrade to PKS v1.4.1 are from PKS v1.4.0 and PKS v1.3.2 and later.
- You can upgrade to PKS v1.4.0 are from PKS v1.3.2 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must 
    increase the size of persistent disk for the PKS VM. For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

## <a id="breaking-changes"></a> Breaking Changes

<%= vars.product_short %> v1.4.x includes the following breaking changes:

### <a id="log-sink"></a> Log Sink Entry Format Change

Log sink entries in PKS v1.4 are tagged with the human-readable cluster name instead of the host ID of the BOSH-defined VM.

### <a id='no-password-om'></a> Passwords Not Supported for Ops Manager VM on vSphere

This breaking change applies to <%= vars.product_short %> v1.4.1 and later.

<%= vars.product_short %> v1.4.1 supports Ops Manager v2.6. 

Starting in Ops Manager v2.6, you can only SSH onto the Ops Manager VM in a vSphere deployment with a private SSH key. You cannot SSH onto the Ops Manager VM with a password.

To avoid upgrade failure and errors when authenticating, add a public key to the **Customize Template** screen of the the OVF template for the Ops Manager VM. Then, use the private key to SSH onto the Ops Manager VM. 

<p class="note warning"><strong>Warning</strong>: You cannot upgrade to Ops Manager v2.6 successfully without adding a public key. If you do not add a key, Ops Manager shuts down automatically because it cannot find a key and may enter a reboot loop.</p>

For more information about adding a public key to the OVF template, see [Deploy Ops Manager](https://docs.pivotal.io/pivotalcf/2-6/om/vsphere/deploy.html#deploy) in _Deploying Ops Manager on vSphere_. 

## <a id="known-issues"></a>Known Issues

<%= vars.product_short %> v1.4.0 has the following known issues:

### <a id="not-enough-diskspace"></a> Upgrade Fails Due to Insufficient Disk Space

**Symptom**

The upgrade from PKS v1.3.x to v1.4.x fails with the following error in the BOSH debug log:

`1 of 8 pre-start scripts failed. Failed Jobs:pxc-mysql`

The pxc-mysql pre-start logs also includes the error message:

`panic: Cannot continue, insufficient disk space to complete migration`

**Explanation**

The upgrade to PKS v1.4.x includes a MySQL migration from MaridDB( Galera) to MySQL (Percona).
This process includes a backup of existing MySQL files.

When `/var/vcap/store` does not have enough space for the backups, the PKS upgrade fails.

**Workaround**

Prior to upgrading to PKS v1.4.x, increase the persistent disk type configured for the PKS VM.

1. In Ops Manager, click on the PKS tile and select **Resource Config**.

1. In **Pivotal Container Service**, select a larger disk type than the currently configured by using the **Persistent Disk Type** dropdown.
For example, if you have 10 GB currently selected, select 20 GB. 
 
If you experience this issue after attempting an upgrade, increase the disk store size and start the upgrade again. 

### <a id="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

### <a id="vm-sizes"></a> PKS-provisioned Kubernetes Cluster Creation Fails in a vSphere with NSX-T Environment 

**Symptom**

Kubernetes cluster creation fails with Enterprise PKS installed on vSphere with NSX-T.

**Explanation**

When configuring a plan for Kubernetes clusters, for the **Worker VM Type** setting, selecting either of the following Worker VM Types results in cluster creation failure because there is not enough disk to handle swap space. 

- `medium (cpu: 2, ram: 4 GM, disk: 8 GB)`
- `medium.mem (cpu: 1, ram: 8 GM, disk: 8 GB)`

BOSH will assign the same amount of RAM to swap up to 50% of the ephemeral disk size. The article [How much Swap Space is Allocated for BOSH-Deployed VMs](https://community.pivotal.io/s/article/How-much-Swap-Space-is-Allocated-for-BOSH-Deployed-VMs) describes the required swap space for VM types. Given the swap space requirements, the `medium` and `medium.mem` Worker VM types are only getting 4GB usable ephemeral disk which is not enough for successful cluster deployment of the PKS tile with NCP. 

**Workaround**

When selecting the Worker VM Type for Worker nodes, do not use the `medium` or `medium.mem` type in a vSphere with NSX-T environment. Use the default Worker VM Type `medium.disk (cpu: 2, ram: 4 GB, disk: 32 GB)` or any other Worker VM Type **except** `medium` or `medium.mem`. 


### <a id="failed-clusters"></a> Upgrade All Service Instances Errand Fails

**Symptom**

After clicking **Apply Changes** in Ops Manager, 
the `upgrade-all-service-instances` errand fails.

**Explanation**

The `upgrade-all-service-instances` errand fails if one or more 
of your existing clusters are in a **FAILED** state.

To check cluster status, run `pks clusters`.

**Workaround**

If you experience this issue, delete and re-create the failed cluster. 
Follow the procedure in 
[Cannot Re-Create a Cluster that Failed to Deploy](troubleshoot-issues.html#cluster-recreate-fails).

### <a id="nsx-t-password-exp"></a> NSX-T Password Expiration Policy

NSX-T v2.4 implements a new <em>password expiration policy</em>. 
By default the administrator password expires after 90 days. If the password expires, 
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, refer to the
following VMware KB article: <a href="https://kb.vmware.com/s/article/70691">https://kb.vmware.com/s/article/70691</a>. 
See also <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the 
<%= vars.product_short %> documentation.</p>

### <a id="nsx-t-nsx-policy"></a> NSX-T Policy API Changes

NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API. 
PKS v1.4.0 does not support the NSX Policy API. Any objects created via the new Policy-based UI cannot be used with PKS v1.4.0.
If you are installing PKS v1.4.0 with NSX-T v2.4.x, or upgrading to PKS v1.4.0 and NSX-T 2.4.x, 
you must use the "Advanced Networking" tab in NSX Manager to create, read, update, and delete 
all networking objects required for PKS. See also <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the 
<%= vars.product_short %> documentation.



