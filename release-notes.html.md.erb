---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.5.0.

## <a id="v1.5.0"></a>v1.5.0

**Release Date**: TBD

### <a id="v1.5.0-features"></a>Features

New features and changes in this release:

* Authorized users can use the PKS CLI command `pks cluster CLUSTER-NAME --details` 
to view details about the named cluster, including Kubernetes node details and NSX-T network details. See the 
[PKS CLI](./cli/index.html) documentation and [Using Network Profiles](./network-profiles.html#network-details).
* Operators can define a network profile that cluster administrators can use to configure the NSX-T
Load Balancer that fronts the Kubernetes API, including using a third-party load balancer. 
See [Load Balancer Configuration](./network-profiles-ncp-lb.html) for details.
* Operators can define a network profile that cluster administrators can use to configure the NCP 
Ingress Controller for managing Pod ingress traffic,including using a third-party ingress controller. 
See [Ingress Controller Configuration](./network-profiles-ncp-ingress.html) for details.
* Operators can define a network profile that cluster administrators can use to configure 
top or bottom section markers for explicit distributed firewall rule placement.
See [DFW Section Marking](./network-profiles-ncp-dfw.html) for details.      
* Operators can define a network profile that cluster administrators can use to tune and extend NCP 
logging and error handling. See [Logging and Error Handling](./network-profiles-ncp-logerr.html) for details.
* Operators can define a network profile that cluster administrators can use to provision Kubernetes 
clusters that implement a shared Tier-1 router topology for NSX-T-based foundations. 
See [Implementing a Shared Tier-1 Topology](./network-profiles-shared-t1.html) for details.
* Operators can define a network profile that cluster administrators can use to configure 
DNS lookup of the Kubernetes API load balancer IP address and ingress controller IP addresses. 
See [Configure DNS lookup Kubernetes API and Ingress Controllers](./network-profiles-dns.html) for details.
* Operators can provision a Windows worker-based Kubernetes cluster on vSphere with Flannel.
Windows worker-based clusters in <%= vars.product_short %> 1.5 currently do not support NSX-T integration.
For more information, see [Configuring Windows Worker-Based Clusters (Beta)](windows-pks-beta.html)
and [Deploying and Exposing Windows Workloads (Beta)](deploy-windows-workloads-beta.html).
* Operators can set the lifetime for the refresh and access tokens for Kubernetes clusters.
You can configure the token lifetimes to meet your organization's security and compliance needs.
For instructions about configuring the access and refresh token for your Kubernetes clusters,
see the [UAA](./installing-pks-vsphere.html#uaa) section in the _Installing_ topic for your IaaS.
* Operators can configure prefixes for OpenID Connect (OIDC) users
and groups to avoid name conflicts with existing Kubernetes system users.
Pivotal recommends adding prefixes to ensure OIDC users and groups
do not gain unintended privileges on clusters. For instructions about configuring OIDC prefixes,
see the [Configure OpenID Connect](./installing-pks-vsphere.html#configure-oidc) section
in the _Installing_ topic for your IaaS.
* Operators can configure an external SAML identity provider for user authentication and authorization.
 For instructions about configuring an external SAML identity provider,
 see the [Configure SAML as an Identity Provider](./installing-pks-vsphere.html#configure-saml)
 section in the _Installing_ topic for your IaaS.
* Operators can upgrade Kubernetes clusters separately from the <%= vars.product_tile %> tile.
For instructions on upgrading Kubernetes clusters, see [Upgrading Clusters](./upgrade-clusters.html).
* Operators can configure the Telgraf agent to send master/etcd node metrics
to a third-party monitoring service. For more information,
see [Monitoring Master/etcd Node VMs](./monitor-etcd.html).
* Operators can configure the default node drain behavior.
You can use this feature to resolve hanging or failed cluster upgrades.
For more information about configuring node drain behavior,
see [Worker Node Hangs Indefinitely](./troubleshoot-issues.html#upgrade-drain-hangs) in _Troubleshooting_
and [Configure Node Drain Behavior](./checklist.html#configure-node-drain)
in Upgrade Preparation Checklist for <%= vars.product_short %> v1.5.
* VMwareâ€™s Customer Experience Improvement Program (CEIP) and the Pivotal Telemetry Program (Telemetry)
are now enabled in <%= vars.product_short %> by default.
This includes both new installations and upgrades.
For information about configuring CEIP and Telemetry in the <%= vars.product_tile %> tile,
see [CEIP and Telemetry](installing-pks-vsphere.html#telemetry) in the _Installing_ topic for your IaaS.
* Authorized Kubernetes cluster users can deploy NSX Service Mesh (NSX-SM), giving you visibility, control, and 
security for services, data, and users at the API level. See [Using NSX-SM with Enterprise PKS](./nsx-sm.html).

### <a id="v1.5.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.5.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>TBD</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions <sup>&#42;</sup> </td>
        <td><%= vars.ops_man_version_2_5 %> or <%= vars.ops_man_version_2_6 %></td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v</td>
    </tr>
    <tr>
        <td>NSX-T versions <sup>&#42;&#42;</sup> </td>
        <td>v2.4.0.1, v2.4.1, v2.4.2, v2.5.0</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.5.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v</td>
    </tr>
</table>

<sup>&#42;</sup> If you want to use Windows workers in <%= vars.product_short %> v1.5,
you must install Ops Manager <%= vars.ops_man_version_2_6 %>. <%= vars.product_short %> does not support
this feature on Ops Manager v2.5. For more information about Ops Manager <%= vars.ops_man_version_2_6 %>,
see [PCF Ops Manager v2.6 Release Notes](https://docs.pivotal.io/pivotalcf/2-6/pcf-release-notes/opsmanager-rn.html#2-6-3).

### <a id="v1.5.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Windows worker-based cluster</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;<sup>&#42;</sup><sup>&#42;</sup></td>
    <td></td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

<sup>&#42;&#42;</sup> Windows worker-based Kubernetes cluster support in <%= vars.product_short %> v1.5 requires vSphere with Flannel.

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.5.0-upgrade"></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.5.0 are from <%= vars.product_short %> v1.4.1 and later.

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

### <a id="v1.5.0-breaking-changes"></a>Breaking Changes

<%= vars.product_short %> v1.5.0 has the following breaking changes:

#### <a name='sink-api-changes'></a> New Sink Resource API Version

The `apps.pivotal.io/v1beta1` sink resource API version is no longer supported. 
The new sink resource API version is `pksapi.io/v1beta1`.  

When creating a sink resource, your sink resource YAML definition must start with `apiVersion: pksapi.io/v1beta1`.
All existing sinks are migrated automatically.  

For more information about defining and managing sink resources, see [Creating Sink Resources](create-sinks.html).

#### <a name='log-sink-changes'></a> Log Sink Changes

<%= vars.product_short %> v1.5.0 adds the following log sink changes:

* The `ClusterSink` and `Sink` log sink resources have been renamed to `ClusterLogSink` and `LogSink` respectively.

    * When creating a log sink resource with YAML, you must use one of the new names in your sink resource YAML definition.
    For example, to define a cluster log sink, specify `kind: ClusterLogSink`.
    All existing sinks are migrated automatically.

    * When managing your log sink resources through kubectl, you must use the new sink resource names.
    For example, if you want to delete a log sink resource, run the `kubectl delete clusterlogsink` command
    instead of `kubectl delete clustersink`.

* Log transport now requires secure connection. When creating a `ClusterLogSink` or `LogSink` resource,
you must include `enable_tls: true` in your sink resource YAML definition.
All existing sinks are migrated automatically.

For more information about defining and managing sink resources, see [Creating Sink Resources](create-sinks.html).

#### <a name='sink-cli-deprecation'></a> Deprecation of Sink Commands in the PKS CLI

The following <%= vars.product_short %> Command Line Interface (PKS CLI) commands are deprecated:

* `pks create-sink`
* `pks sinks`
* `pks delete-sink`

<%= vars.product_short %> v1.6 removes these commands.
For more information about defining and managing sink resources,
see [Creating Sink Resources](create-sinks.html).

### <a id="v1.5.0-known-issues"></a> Known Issues

<%= vars.product_short %> v1.5.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, on Azure the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after upgrading <%= vars.product_short %>.

**Explanation**

<%= vars.product_short %> uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a name='no-password-om'></a> Passwords Not Supported for Ops Manager VM on vSphere

Starting in Ops Manager v2.6, you can only SSH onto the Ops Manager VM in a vSphere deployment with a private SSH key. You cannot SSH onto the Ops Manager VM with a password.

To avoid upgrade failure and errors when authenticating, add a public key to the **Customize Template** screen of the the OVF template for the Ops Manager VM. Then, use the private key to SSH onto the Ops Manager VM.

<p class="note warning"><strong>Warning</strong>: You cannot upgrade to Ops Manager v2.6 successfully without adding a public key. If you do not add a key, Ops Manager shuts down automatically because it cannot find a key and may enter a reboot loop.</p>

For more information about adding a public key to the OVF template, see [Deploy Ops Manager](https://docs.pivotal.io/pivotalcf/2-6/om/vsphere/deploy.html#deploy) in _Deploying Ops Manager on vSphere_.

#### <a name='oidc'></a> New OIDC Prefixes Break Existing Cluster Role Bindings

In <%= vars.product_short %> v1.5, operators can configure prefixes for OIDC usernames and groups.
If you add OIDC prefixes you must manually change any existing role bindings that bind to a username or group.
If you do not change your role bindings, developers cannot access Kubernetes clusters.
For instructions about creating a role binding, see [Managing Cluster Access and Permissions](./manage-cluster-permissions.html).

#### <a name='timeout'></a> Timeout Error During Individual Cluster Upgrades

**Symptom**

A timeout error occurs when you upgrade your Kubernetes clusters individually using the `pks upgrade-cluster` command.

**Explanation**

BOSH upgrades Kubernetes clusters in parallel with a limit of four concurrent cluster upgrades.
If you schedule more than four cluster upgrades,
<%= vars.product_short %> queues the upgrades and waits for BOSH to finish the last upgrade.
When BOSH finishes the last upgrade, it starts working on the next upgrade request.

If you submit too many cluster upgrades to BOSH, a timeout error may occur. The timeout is set to 168 hours.
However, BOSH does not remove the task from the queue or stop working on the upgrade if it has been picked up.

**Solution**

If you expect that upgrading all of your Kubernetes clusters takes more than 168 hours,
do not use a script that submits upgrade requests for all of your clusters at once.
For information about upgrading Kubernetes clusters provisioned by <%= vars.product_short %>,
see [Upgrading Clusters](./upgrade-clusters.html).
