---
title: PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for Pivotal Container Service (PKS) v1.3.x.

---
## <a id="v1.3.6"></a>v1.3.6

**Release Date**: April 8, 2019

### <a id="v1.3.6-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.6</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 8, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.7</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.11</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1, v2.4.0.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.2</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.11</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id='v1.3.6-vsphere-reqs'></a> vSphere Version Requirements

If you are installing PKS on vSphere or vSphere with NSX&#8209;T, note that Ops Manager and PKS support the following vSphere component versions:
<%= partial 'vsphere_versions' %>

### <a id="v1.3.6-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.6-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.6 are as follows:

- PKS v1.3.4 or later

### <a id="v1.3.6-whats-new"></a>What's New

PKS v1.3.6 adds the following:

- Telemetry property `environment_provider`.
- Support for `nsx-cf-cni` with 2.4.0.12511604.
- Remaining plans to the osb-proxy configuration.

### <a id="v1.3.6-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.6 has the following known issues:

####<a id="small-ephemeral-disks"></a> NSX-T Upgrades from v2.3.X to v2.4.0.1 Fail for Bare Metal Edge Node.

Upgrading NSX-T v2.3.X to v2.4.0.1 fails for Bare Metal Edge Nodes. 

If you are using a Bare Metal Edge Nodes, please refrain from upgrading NSX-T v2.3.x to NSX-T v2.4.0.1.

####<a id="small-ephemeral-disks"></a> Worker Nodes with Small Ephemeral Disks Can Cause Upgrade Failure

PKS deploys packages to the ephemeral disk, `/var/vcap/data`, during installations and upgrades. If master or worker node VMs have ephemeral disks smaller than 8&nbsp;GB,  the disk can fill during an upgrade and cause the upgrade to fail. Cluster upgrades can present error messages such as the following:

```
{"time":999999999,"error":{"code":450001,"message":"Response exceeded maximum allowed length"}}
```

**Workaround**: In the plans you use to deploy clusters, ensure that worker and master node ephemeral disks are set to greater than 8&nbsp;GB. For plan configuration instructions, see the [Plans](./installing-nsx-t.html#plans) section of the _Installing PKS_ topic for your IaaS.

This issue should not affect new installations of PKS v1.3.x as the default ephemeral disk size in plans is larger than 8&nbsp;GB.

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

**Workaround**: If you install PKS on Azure and experience this issue, contact Support for assistance.

####<a id="kubelet-customizations"></a> The kubelet customization feature is only enabled for Plan 1

PKS 1.3.4 introduces the ability to configure kubelet startup parameters `system-reserved` and `evction-hard` within a [plan](https://docs.pivotal.io/runtimes/pks/1-3/installing-pks-vsphere.html#plans).  This capability is only functional in Plan 1 for PKS 1.3.4 and will be enabled in additional plans in the next release.

## <a id="v1.3.5"></a>v1.3.5

**Release Date**: March 28, 2019

### <a id="v1.3.5-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.5</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>March 28, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.7</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.11</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.2</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.11</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id='v1.3.5-vsphere-reqs'></a> vSphere Version Requirements

If you are installing PKS on vSphere or vSphere with NSX&#8209;T, note that Ops Manager and PKS support the following vSphere component versions:
<%= partial 'vsphere_versions' %>

### <a id="v1.3.5-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.5-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.5 are as follows:

- PKS v1.3.4 or later

### <a id="v1.3.5-whats-new"></a>What's New

PKS v1.3.5 adds the following:

- Support for Kubernetes v1.12.7.
- **Fix**: [CVE-2019-1002101](https://nvd.nist.gov/vuln/detail/CVE-2019-1002101). Kubernetes v1.12.7 address this CVE.
- **Fix**: [CVE-2019-9946](https://github.com/kubernetes/kubernetes/pull/75455). Kubernetes v1.12.7 address this CVE.

### <a id="v1.3.5-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.5 has the following known issues:

####<a id="small-ephemeral-disks"></a> Worker Nodes with Small Ephemeral Disks Can Cause Upgrade Failure

PKS deploys packages to the ephemeral disk, `/var/vcap/data`, during installations and upgrades. If master or worker node VMs have ephemeral disks smaller than 8&nbsp;GB,  the disk can fill during an upgrade and cause the upgrade to fail. Cluster upgrades can present error messages such as the following:

```
{"time":999999999,"error":{"code":450001,"message":"Response exceeded maximum allowed length"}}
```

**Workaround**: In the plans you use to deploy clusters, ensure that worker and master node ephemeral disks are set to greater than 8&nbsp;GB. For plan configuration instructions, see the [Plans](./installing-nsx-t.html#plans) section of the _Installing PKS_ topic for your IaaS.

This issue should not affect new installations of PKS v1.3.x as the default ephemeral disk size in plans is larger than 8&nbsp;GB.

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

**Workaround**: If you install PKS on Azure and experience this issue, contact Support for assistance.

####<a id="kubelet-customizations"></a> The kubelet customization feature is only enabled for Plan 1

PKS 1.3.4 introduces the ability to configure kubelet startup parameters `system-reserved` and `evction-hard` within a [plan](https://docs.pivotal.io/runtimes/pks/1-3/installing-pks-vsphere.html#plans).  This capability is only functional in Plan 1 for PKS 1.3.4 and will be enabled in additional plans in the next release.

---
## <a id="v1.3.4"></a>v1.3.4

**Release Date**: March 26, 2019

### <a id="v1.3.4-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.4</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>March 26, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.6</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.11</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.2</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.11</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id='v1.3.4-vsphere-reqs'></a> vSphere Version Requirements

If you are installing PKS on vSphere or vSphere with NSX&#8209;T, note that Ops Manager and PKS support the following vSphere component versions:
<%= partial 'vsphere_versions' %>

### <a id="v1.3.4-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.4-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.3 are as follows:

- **When upgrading from PKS v1.3.x**: PKS v1.3.1 or later
- **When upgrading from PKS v1.2.x**: PKS v1.2.8 or later

### <a id="v1.3.4-whats-new"></a>What's New

PKS v1.3.4 adds the following:

- Custom DNS configuration for Kubernetes clusters using NSX-T and Network Profiles. For more information, see [DNS Configuration for Kubernetes Clusters] (./network-profiles-define.html#dns) in _Defining Network Profiles_.
- Support for NSX-T NCP v2.3.2. For more information, see the [VMware NSX Container Plug-in 2.3.2 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3.2/rn/VMware-NSX-T-Data-Center-232-Release-Notes.html).
- Support for additional plans. Operators can configure up to ten sets of resource types, or **Plans**, in the PKS tile. All plans except the first can made available or unavailable to developers deploying clusters. **Plan 1** must be configured and made available as a default for developers.
- Kubelet customization. You can enable Kubelet to reserve compute resources for system daemons by configuring the startup parameters `system-reserved` and `eviction-hard` in the **Plans** pane of the PKS tile. For more information, see the *Plans* section of the *Installing PKS* topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#plans).
- **Fix**: [CVE-2019-1002100](https://discuss.kubernetes.io/t/kubernetes-security-announcement-v1-11-8-1-12-6-1-13-4-released-to-address-medium-severity-cve-2019-1002100/5147). Kubernetes v1.12.6 address this CVE.
- **Fix**: Updated the Telemetry URL.
- **Fix**: Resolved an issue where vSphere Cloud Provider configuration could fail if credentials contained non-alphanumeric characters. For example, `#`, `\`, and `"`.

### <a id="v1.3.4-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.4 has the following known issues:

####<a id="small-ephemeral-disks"></a> Master and Worker Nodes with Small Ephemeral Disks Can Cause Upgrade Failure

PKS deploys packages to the ephemeral disk, `/var/vcap/data`, during installations and upgrades. If master and worker node VMs have ephemeral disks smaller than 8&nbsp;GB, the disk can fill during an upgrade and cause the upgrade to fail. Cluster upgrades can present error messages such as the following:

```
{"time":999999999,"error":{"code":450001,"message":"Response exceeded maximum allowed length"}}
```

**Workaround**: In the plans you use to deploy clusters, ensure that the master and worker node ephemeral disks are set to greater than 8&nbsp;GB. For plan configuration instructions, see the *Plans* section of the _Installing PKS_ topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#plans).

This issue should not affect new installations of PKS v1.3.x as the default ephemeral disk size in plans is larger than 8&nbsp;GB.

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

**Workaround**: If you install PKS on Azure and experience this issue, contact Support for assistance.

####<a id="kubelet-customizations"></a> Kubelet Customization Feature Only Enabled for Plan 1

PKS v1.3.4 introduces the ability to configure Kubelet startup parameters `system-reserved` and `eviction-hard` within a plan. For more information, see the *Plans* section of the *Installing PKS* topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#plans).

This feature is only functional in Plan 1 for PKS v1.3.4 and will be enabled in additional plans in the next release.

## <a id="v1.3.3"></a>v1.3.3

**Release Date**: February 22, 2019

### <a id="v1.3.3-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.3</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>February 22, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.9</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.9</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id="v1.3.3-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.3-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.3 are as follows:

- **When upgrading from PKS v1.3.x**: PKS v1.3.1 or v1.3.2
- **When upgrading from PKS v1.2.x**: PKS v1.2.8 through v1.2.11

### <a id="v1.3.3-whats-new"></a>What's New

PKS v1.3.3 adds the following:

- **Fix**: [CVE-2019-5736](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5736). This release updates the version of Docker deployed by PKS to v18.06.3-ce. This Docker version addresses a runc vulnerability whereby a malicious image could run in privileged mode and elevate to root access on worker nodes. Docker v18.06.2-ce, deployed by PKS v1.3.2, did not contain the correct compiled binary. This Docker version includes the correct runc binary to address the CVE.

### <a id="v1.3.3-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.3 has the following known issues:

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="special-char"></a> Deploy Fails if vSphere Master Credentials Field Has Special Characters Without Quotes

If you install PKS on vSphere and you enter credentials in the **vCenter Master Credentials** field of the **Kubernetes Cloud Provider** pane of the PKS tile that contain special characters, such as `#`, `$`, `,`, `!`, or `-`, your deployment might fail with the following error:

```
ServerFaultCode: Cannot complete login due to an incorrect user name or password.
```

**Workaround**: If you install PKS on vSphere **without** NSX-T integration, place quotes around the credentials in the cloud provider configuration. For example, `"SomeP4$$w0rd#!"`. Then redeploy the PKS tile by clicking **Apply Changes**.

If you install PKS on vSphere **with** NSX-T integration, avoid using special characters in this field until this issue is resolved.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

**Workaround**: If you install PKS on Azure and experience this issue, contact Support for assistance.

## <a id="v1.3.2"></a>v1.3.2

**Release Date**: February 13, 2019

### <a id="v1.3.2-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.2</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>February 13, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.4</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.8</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.2-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.8</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id="v1.3.2-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.2-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.2 are as follows:

- **When upgrading from PKS v1.3.x**: PKS v1.3.1
- **When upgrading from PKS v1.2.x**: PKS v1.2.8 or v1.2.9

### <a id="v1.3.2-whats-new"></a>What's New

PKS v1.3.2 adds the following:

- **Fix**: [CVE-2019-3779](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-3779). This fix addresses a vulnerability where certs signed by the Kubernetes API could be used to gain access to a PKS-deployed cluster's etcd service.
- **Fix**: [CVE-2019-3780](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-3780). This fixes a regression bug in PKS where vCenter IaaS credentials intended for the vSphere Cloud Provider were written on worker node VM disks.
- **Fix**: Clusters can now be successfully created if there are pre-existing Kubernetes clusters using the same hostname.

### <a id="v1.3.2-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.2 has the following known issues:

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="special-char"></a> Deploy Fails if vSphere Master Credentials Field Has Special Characters Without Quotes

If you install PKS on vSphere and you enter credentials in the **vCenter Master Credentials** field of the **Kubernetes Cloud Provider** pane of the PKS tile that contain special characters, such as `#`, `$`, `,`, `!`, or `-`, your deployment might fail with the following error:

```
ServerFaultCode: Cannot complete login due to an incorrect user name or password.
```

**Workaround**: If you install PKS on vSphere **without** NSX-T integration, place quotes around the credentials in the cloud provider configuration. For example, `"SomeP4$$w0rd#!"`. Then redeploy the PKS tile by clicking **Apply Changes**.

If you install PKS on vSphere **with** NSX-T integration, avoid using special characters in this field until this issue is resolved.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to either PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

**Workaround**: If you install PKS on Azure and experience this issue, contact Support for assistance.

## <a id="v1.3.1"></a>v1.3.1

**Release Date**: February 8, 2019

<p class="note warning"><strong>WARNING</strong>: PKS v1.3.1 and earlier includes a critical CVE. Follow the procedures in the <a href="https://community.pivotal.io/s/article/pks-upgrade-approach-for-critical-cve">PKS upgrade approach for CRITICAL CVE</a> article in the Pivotal Support Knowledge Base to perform an upgrade to PKS v1.3.2.</p>

### <a id="v1.3.1-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>February 8, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.4</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>CFCR</td>
        <td>v0.25.8</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.1-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR v0.25.8</a></td>
    </tr>
</table>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id='v1.3.1-vsphere-reqs'></a> vSphere Version Requirements

If installing PKS on vSphere or vSphere with NSX&#8209;T, please note Ops Manager and PKS support the following vSphere component versions:
<%= partial '_vsphere_versions' %>

### <a id="v1.3.1-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

### <a id="v1.3.1-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.1 are as follows:

- **When upgrading from PKS v1.3.x**: PKS v1.3.0
- **When upgrading from PKS v1.2.x**: PKS v1.2.7 or v1.2.8

Follow the procedures in the <a href="https://community.pivotal.io/s/article/pks-upgrade-approach-for-critical-cve">PKS upgrade approach for CRITICAL CVE</a> article in the Pivotal Support Knowledge Base to perform an upgrade to PKS v1.3.2.

### <a id="v1.3.1-whats-new"></a>What's New

PKS v1.3.1 adds support for the following:

- Certificates for the Etcd instance for each Kubernetes cluster provisioned by PKS are generated with a four-year lifetime and signed by a new Etcd Certificate Authority (CA).
* **Fix**: Upgrading PKS no longer fails during upgrades if there are Kubernetes clusters with duplicate hostnames.
* **Fix**: Deploying PKS no longer fails if an entry in the **No Proxy** field contains special characters such as (`-`) character.
* **Fix**: The Kubernetes API now responds with the CA certificate that signed the Kubernetes cluster's certificate so that customer scripts such as the [get-pks-k8s-config.sh](https://community.pivotal.io/s/article/script-to-automate-generation-of-the-kubeconfig-for-the-kubernetes-user) tool will function again.

### <a id="v1.3.1-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3.x, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.1 has the following known issues:

####<a id="flannel"></a> PKS Flannel Network Gets out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="special-char"></a> Deploy Fails if vSphere Master Credentials Field Has Special Characters Without Quotes

If you install PKS on vSphere and you enter credentials in the **vCenter Master Credentials** field of the **Kubernetes Cloud Provider** pane of the PKS tile that contain special characters, such as `#`, `$`, `,`, `!`, or `-`, your deployment might fail with the following error:

```
ServerFaultCode: Cannot complete login due to an incorrect user name or password.
```

**Workaround**: If you install PKS on vSphere **without** NSX-T integration, place quotes around the credentials in the cloud provider configuration. For example, `"SomeP4$$w0rd#!"`. Then redeploy the PKS tile by clicking **Apply Changes**.

If you install PKS on vSphere **with** NSX-T integration, avoid using special characters in this field until this issue is resolved.

####<a id="azure-service"></a> Cluster Upgrades from PKS v1.3.0 on Azure Fail If Services Are Exposed

If you install PKS v1.3.0 on Azure, clusters might fail with the following error when you upgrade to either PKS v1.3.1 or later:

```
result: 1 of 2 post-start scripts failed. Failed Jobs: kubelet. Successful Jobs: bosh-dns
```

This issue is caused by a timeout condition. The issue affects nodes hosting Kubernetes pods that are exposed externally by a Kubernetes service.

New cluster creations and cluster scaling operations are not affected by this issue.

## <a id="v1.3.0"></a>v1.3.0

**Release Date**: January 16, 2019

<p class="note warning"><strong>WARNING</strong>: PKS v1.3.0 has a known vulnerability and is no longer available. Install or upgrade to <a href="release-notes.html#v1.3.1">PKS v1.3.1</a>.</p>

### <a id="v1.3.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.3.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>January 16, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.3.1+, v2.4.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v170.15</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.12.4</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.1-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
</table>

<strong><sup>&#42;</sup></strong> PKS v1.3 supports NSX-T v2.2 and v2.3 with the following caveats:

<ul>
    <li>To use the network profile features available in PKS v1.3, you must use NSX-T v2.3.</li>
    <li>NSX-T 2.3.0 has a known critical issue: <a href="https://kb.vmware.com/s/article/60293">ESX hosts lose network connectivity rendering the host inaccessible from network (60293)</a>. This issue is patched in NSX-T v2.3.0.2 and fixed in the NSX-T v2.3.1 release.</li>
</ul>

<p class="note"><strong>Note:</strong> Ops Manager v2.3.10 and later in the
v2.3 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.3 on Azure.
Before deploying PKS v1.3 on Azure, you must install Ops Manager v2.3.9 or
earlier in the 2.3 version line or Ops Manager v2.4.3 or earlier in the 2.4
version line.</p>

### <a id='v1.3.0-vsphere-reqs'></a> vSphere Version Requirements

If installing PKS on vSphere or vSphere with NSX&#8209;T, please note Ops Manager and PKS support the following vSphere component versions:
<%= partial '_vsphere_versions' %>

### <a id="v1.4.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Accessing Basic Workloads_.

### <a id="v1.3.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.3.0 are from PKS v1.2.5 and later.

For more information, see [Upgrading PKS](upgrade-pks.html) and [Upgrading PKS with NSX-T](upgrade-pks-nsxt.html).

<p class="note"><strong>Note</strong>: Upgrading from PKS v1.2.5+ to PKS v1.3.x causes all certificates to be automatically regenerated. The old certificate authority is still trusted, and has a validity of one year. But the new certificates are signed with a new certificate authority, which is valid for four years.</p>

### <a id="v1.3.0-whats-new"></a>What's New

PKS v1.3.0 adds the following:

- Support for PKS on Azure. For more information, see [Azure](azure-index.html).
- BOSH Backup and Restore (BBR) for single-master clusters. For more information, see [Backing up the Single Master Cluster](bbr-backup-cluster.html) and [Restoring the Single Master Cluster](bbr-restore-cluster.html).
- Routable pods on NSX-T. For more information, see [Routable Pod Networks](network-profiles-define.html#routable-pods) in <em>Defining Network Profiles</em>.
- Large size NSX-T load balancers with Bare Metal NSX-T edge nodes. For more information, see [Hardware Requirements for PKS on vSphere with NSX-T](vsphere-nsxt-rpd-mpd.html#pks-edge-cluster).
- HTTP proxy for NSX-T components. For more information, see [Using Proxies with PKS on NSX-T](proxies.html).
- Ability to specify the size of the Pods IP Block subnet using a network profile. For more information, see [Pod Subnet Prefix](./network-profiles-define.html#pod-prefix) in <em>Defining Network Profiles</em>.
- Support for bootstrap security groups, custom floating IPs, and edge router selection using network profiles. For more information, see [Bootstrap Security Group](network-profiles-define.html#ns-groups), [Custom Floating IP Pool](network-profiles-define.html#floating-ip), and [Edge Router Selection](network-profiles-define.html#multi-t0) in <em>Defining Network Profiles</em>.
- Support for sink resources in air-gapped environments.
- Support for creating sink resources with the PKS Command Line Interface (PKS CLI). For more information, see [Creating Sink Resources](create-sinks.html).
- Sink resources include both pod logs as well as events from the Kubernetes API. These events are combined in a shared format that provides operators with a robust set of filtering and monitoring options. For more information, see [Monitoring PKS with Sinks](monitor-sinks.html).
- Support for multiple NSX-T Tier-0 (T0) logical routers for use with PKS multi-tenant environments. For more information, see [Configuring Multiple Tier-0 Routers for Tenant Isolation](nsxt-multi-t0.html).
- Support for multiple PKS foundations on the same NSX-T. For more information, see [Implementing a Multi-Foundation PKS Deployment](nsxt-multi-pks.html).
- Smoke tests errand that uses the PKS CLI to create a Kubernetes cluster and then delete it. If the creation or deletion fails, the errand fails and the installation of the PKS tile is aborted. For more information, see the <em>Errands</em> section of the <em>Installing PKS</em> topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#errands).
- Support for scaling down the number of worker nodes. For more information, see [Scaling Existing Clusters](scale-clusters.html).
- Support for defining the CIDR range for Kubernetes pods and services on Flannel networks. For more information, see the <em>Networking</em> section of the <em>Installing PKS</em> topic for your IaaS, such as [Installing PKS on vSphere](installing-pks-vsphere.html#networking).
- Kubernetes v1.12.4.
- **Bug Fix**: The **No Proxy** property for vSphere now accepts wildcard domains like
`*.example.com` and `example.com`. See [Networking](./installing-pks-vsphere.html#networking) in
_Installing PKS on vSphere_ for more information.
- **Bug Fix**: The issue with NSX-T where special characters in username and password doesn't work is resolved.
- **Security Fix**: [CVE 2018-18264](https://pivotal.io/security/cve-2018-18264): This CVE allows unauthenticated secret access to the Kubernetes Dashboard.
- **Security Fix**: [CVE-2018-15759](https://pivotal.io/security/cve-2018-15759): This CVE contains an insecure method of verifying credentials. A remote unauthenticated malicious user may make many requests to the service broker with a series of different credentials, allowing them to infer valid credentials and gain access to perform broker operations.

### <a id="v1.3.0-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking Change:</strong> Heapster is deprecated in PKS v1.3, and
Kubernetes has retired Heapster. For more information, see the
<a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
repository on GitHub.</p>

PKS v1.3.0 has the following known issues:

#### <a id="duplicate-external-host"></a>Upgrades Fail When Clusters Share an External Hostname

If you use the same external hostname across more than one PKS-deployed Kubernetes
cluster, upgrades from PKS v1.2.x to PKS v1.3.0 might fail. The external hostname is
the value you set with either the `-e` or `--external-hostname` argument when you
created the cluster. For more information, see [Create a Kubernetes Cluster](./create-cluster.html#create).

PKS v1.3.0 introduces restrictions that prevent you from deploying clusters with
duplicate hostnames, so this issue does not affect upgrades from PKS v1.3.0 and
later.

If you have existing clusters that use the same external hostname, do not upgrade
to PKS v1.3.x. Contact your Support representative for more information.

#### <a id="no-proxy"></a>Upgrades Fail with a Hyphen in the No Proxy Field on vSphere

If you install PKS on vSphere and you enable the **HTTP/HTTPS Proxy** setting, you
cannot use the `-` character in the **No Proxy** field. Entering `-` in the **No
Proxy** field can cause validation errors when trying to upgrade to PKS v1.3.0.
For more information, see the [Networking](./installing-pks-vsphere.html#networking) section of
_Installing PKS on vSphere_.

If you experience this issue during an upgrade, contact Support for a hotfix
that will be applied in a future PKS v1.3.x release.

####<a id="flannel"></a> PKS Flannel Network Gets Out of Sync with Docker Bridge Network (cni0)

When VMs have been powered down for multiple days, turning them back on and issuing a `bosh recreate` to re-create the VMs causes the pods to get stuck in a `ContainerCreating` state.

**Workaround**: See [PKS Flannel network gets out of sync with docker bridge network (cni0)](https://community.pivotal.io/s/article/PKS-Flannel-network-gets-out-of-sync-with-docker-bridge-network-cni0) in the Pivotal Knowledge Base.

####<a id="special-char"></a> Deploy Fails if vSphere Master Credentials Field Has Special Characters Without Quotes

If you install PKS on vSphere and you enter credentials in the **vCenter Master Credentials** field of the **Kubernetes Cloud Provider** pane of the PKS tile that contain special characters, such as `#`, `$`, `,`, `!`, or `-`, your deployment might fail with the following error:

```
ServerFaultCode: Cannot complete login due to an incorrect user name or password.
```

**Workaround**: If you install PKS on vSphere **without** NSX-T integration, place quotes around the credentials in the cloud provider configuration. For example, `"SomeP4$$w0rd#!"`. Then redeploy the PKS tile by clicking **Apply Changes**.

If you install PKS on vSphere **with** NSX-T integration, avoid using special characters in this field until this issue is resolved.
