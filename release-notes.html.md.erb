---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.4.x.

## <a id="v1.4.0"></a>v1.4.0

**Release Date**: 

### <a id="v1.4.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.0+</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.x</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.24</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.2, v2.3.0.2, v2.3.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.3.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.1-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
</table>

<strong><sup>&#42;</sup></strong> <%= vars.product_short %> v1.4 supports NSX-T v2.2 and v2.3 with the following restrictions:

<ul> 
    <li>To use the network profile features available in <%= vars.product_short %> v1.4, you must use NSX-T v2.3.</li>
    <li>NSX-T 2.3.0 has a known critical issue: <a href="https://kb.vmware.com/s/article/60293">ESX hosts lose network connectivity rendering the host inaccessible from network (60293)</a>. This issue is patched in NSX-T v2.3.0.2 and fixed in the NSX-T v2.3.1 release.</li>
</ul>

<p class="note"><strong>Note:</strong> Ops Manager v2.5.0 and later in the
v2.5 version line and Ops Manager v2.4.4 and later in the v2.4 version line
do not support PKS v1.4 on Azure.
Before deploying PKS v1.4 on Azure, you must install Ops Manager v2.4.2
or v2.4.3.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> on vSphere or vSphere with NSX&#8209;T installatios, Ops Manager and <%= vars.product_short %> support the following vSphere component versions:
<%= partial '_vsphere_versions' %>

### <a id="v1.4.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Accessing Basic Workloads_.

### <a id="v1.4.0-upgrade"></a>Upgrade Path

For more information, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

### <a id="v1.4.0-whats-new"></a>What's New

<%= vars.product_short %> v1.4.0 adds the following:

* Operators can configure up to ten sets of resource types, or _plans_, in the PKS tile. All
plans except the first can made available or unavailable to developers deploying clusters. Plan 1
must be configured and made available as a default for developers. For more information, see the
_Plans_ section of the Installing <%= vars.product_short %> documentation for your IaaS.

* Operators can install PKS and Pivotal Application Service (PAS) on the same instance of Ops
Manager.

* Improved workflow for managing cluster access. For more information, see [Grant Cluster Access](manage-users.html#cluster-access) in _Managing Users in <%= vars.product_short %> with UAA_.

* Operators can create webhook ClusterSink resources. A webhook ClusterSink resource batches logs into 1 second units, wraps the resulting payload in JSON, and uses the POST method to deliver the logs to the address of your log management service. For more information see, [Create a Webhook ClusterSink Resource with YAML and kubectl](create-sinks.html#webhook-cluster-sink) in _Creating Sinks_.

* Operators can set quotas for maximum memory and CPU utilization in a PKS deployment. For more information, see the _PKS API_ section of the Installing Enterprise PKS documentation for your IaaS. This is a beta feature.

    <%= partial 'beta-component' %>

### <a id="v1.4.0-known-issues"></a>Breaking Changes and Known Issues

<%= vars.product_short %> v1.4.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of the `LoadBalancer` type.
Additionally, in the Azure portal, the **Networking** tab of each cluster VM does not display
any inbound and outbound traffic rules.

**Explanation**

When configuring the **Kubernetes Cloud Provider** pane of the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group**.
Because this security group is associated with your Kubernetes cluster subnet,
<%= vars.product_short %> automatically assigns **Default Security Group** to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.
