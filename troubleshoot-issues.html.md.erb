---
title: Troubleshooting
owner: PKS
---

<strong><%= modified_date %></strong>

##<a id='api-timeout'></a>PKS API is Slow or Times Out

**Symptom**

When you run PKS CLI commands, the PKS API times out or is slow to respond.

**Explanation**

The PKS API control plane VM requires more resources.

**Solution**

1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.

1. Select the **<%= vars.product_tile %>** tile.

1. Select the **Resource Config** page.

1. For the **Pivotal Container Service** job, select a **VM Type** with greater CPU and memory resources.

1. Click **Save**.

1. Click the **Installation Dashboard** link to return to the Installation Dashboard.

1. Click **Review Pending Changes**. Review the changes that you made. For more
information, see [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/customizing/review-pending-changes.html).
1. Click **Apply Changes**.

##<a id='cluster-create-fail'></a>Cluster Creation Fails

**Symptom**

When creating a cluster, you run `pks cluster CLUSTER-NAME` to monitor the cluster creation status.
In the command output, the value for **Last Action State** is `error`.

**Explanation**

There was an error creating the cluster.

**Diagnostics**

1. Log in to the BOSH Director and run `bosh tasks`.
  The output from `bosh tasks` provides details about the tasks that the BOSH Director has run.
  See [Managing <%= vars.product_short %> Deployments with BOSH](manage-deployments-bosh.html) for more information about logging in to the BOSH Director.

1. In the BOSH command output, locate the task that attempted to create the cluster.

1. To retrieve more information about the task, run the following command:

    ```
    bosh -e MY-ENVIRONMENT task TASK-NUMBER
    ```
    Where:
    * `MY-ENVIRONMENT` is the name of your BOSH environment.
    * `TASK-NUMBER` is the number of the task that attempted to create the cluster.
    <br>
    For example:
    <pre class="terminal">$ bosh -e pks task 23</pre>

BOSH logs are used for error diagnostics but if the issue you see in the BOSH logs is related to using or managing Kubernetes, you should consult the [Kubernetes Documentation](https://kubernetes.io/docs/home/?path=users&persona=app-developer&level=foundational) for troubleshooting that issue.

For troubleshooting failed BOSH tasks, see the [BOSH documentation](https://bosh.io/docs/cli-v2#task-mgmt).

##<a id='cluster-delete-fail'></a>Cluster Deletion Fails

**Symptom**

When attempting to delete a cluster using `pks delete-cluster CLUSTER-NAME`, the cluster is not deleted.

**Explanation**

There was an error deleting the cluster.

**Solution**

Log in to the BOSH Director and delete the BOSH deployment manually, then retry the `pks delete-cluster` operation.

1. Log in to the BOSH Director and obtain the deployment name for cluster you want to delete. For instructions, see [Managing <%= vars.product_short %> Deployments with BOSH](manage-deployments-bosh.html).

1. Run the following BOSH command:

    ```
    bosh -e MY-ENVIRONMENT delete-deployment -d DEPLOYMENT-NAME
    ```
    Where:
    * `MY-ENVIRONMENT` is the name of your BOSH environment.
    * `DEPLOYMENT-NAME` is the name of your BOSH deployment.
    <p class="note"><strong>Note</strong>: If necessary, you can append the <code>--force</code> flag to delete the deployment.</p>

2. Run the following PKS command:

    ```
    pks delete-cluster CLUSTER-NAME
    ```
    Where `CLUSTER-NAME` is the name of your <%= vars.product_short %> cluster.

##<a id='cluster-recreate-fails'></a>Cannot Re-Create a Cluster that Failed to Deploy

**Symptom**

After cluster creation fails, you cannot re-run `pks create-cluster` to attempt
creating the cluster again.

**Explanation**

<%= vars.product_short %> does not automatically clean up the failed BOSH deployment. Running `pks
create-cluster` using the same cluster name creates a name clash error in BOSH.

**Solution**

Log in to the BOSH Director and delete the BOSH deployment manually, then retry the `pks delete-cluster` operation. After cluster deletion succeeds, re-create the cluster.

1. Log in to the BOSH Director and obtain the deployment name for cluster you want to delete. For instructions, see [Managing <%= vars.product_short %> Deployments with BOSH](manage-deployments-bosh.html).

1. Run the following BOSH command:

    ```
    bosh -e MY-ENVIRONMENT delete-deployment -d DEPLOYMENT-NAME
    ```
    Where:
    * `MY-ENVIRONMENT` is the name of your BOSH environment.
    * `DEPLOYMENT-NAME` is the name of your BOSH deployment.
    <p class="note"><strong>Note</strong>: If necessary, you can append the <code>--force</code> flag to delete the deployment.</p>

2. Run the following PKS command:

    ```
    pks delete-cluster CLUSTER-NAME
    ```
    Where `CLUSTER-NAME` is the name of your <%= vars.product_short %> cluster.

1. To re-create the cluster, run the following PKS command:

    ```
    pks create-cluster CLUSTER-NAME
    ```
    Where `CLUSTER-NAME` is the name of your <%= vars.product_short %> cluster.

##<a id="timeouts"></a>Cannot Access Add-On Features or Functions

**Symptom**

You cannot access a feature or function provided by a Kubernetes add-on.

Examples include the following:

- You cannot access the Kubernetes [Web UI (Dashboard)](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) in a browser or using the kubectl command-line tool.
- Pods cannot resolve DNS names, and error messages report the service `kube-dns` is invalid. If `kube-dns` is not deployed, the cluster typically fails to start.
- [Heapster](https://github.com/kubernetes/heapster) does not start.

**Explanation**

The Kubernetes features and functions listed above are provided by the following <%= vars.product_short %> add-ons:

* **Kubernetes Dashboard** `kubernetes-dashboard`
* **DNS Resolution**: `kube-dns`
* **Heapster**: `heapster`
  <p class='note'><strong>Note:</strong> Heapster is deprecated in <%= vars.product_short %> v1.3, and Kubernetes has
  retired Heapster. For more information, see the
  <a href="https://github.com/kubernetes-retired/heapster">kubernetes-retired/heapster</a>
  repository in GitHub.</p>

To enable these add-ons, Ops Manager must run scripts after deploying <%= vars.product_short %>. You must configure Ops Manager to automatically run these post-deploy scripts.

**Solution**

Perform the following steps to configure Ops Manager to run post-deploy scripts to deploy the missing add-ons to your cluster.

1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.

1. Click the **BOSH Director** tile.

1. Select **Director Config**.

1. Select **Enable Post Deploy Scripts**.
  <p class="note"><strong>Note</strong>: This setting enables post-deploy scripts for all tiles in your Ops Manager installation.</p>

1. Click **Save**.

1. Click the **Installation Dashboard** link to return to the Installation Dashboard.

1. Click **Review Pending Changes**. Review the changes that you made. For more
information, see [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/customizing/review-pending-changes.html).

1. Click **Apply Changes**.

1. After Ops Manager finishes applying changes, enter `pks delete-cluster` on the command line to delete the cluster. For more information, see [Deleting Clusters](delete-cluster.html).

1. On the command line, enter `pks create-cluster` to recreate the cluster. For more information, see [Creating Clusters](create-cluster.html).

##<a id='vsphere-ha-permissions'></a>Resurrecting VMs Causes Incorrect Permissions in vSphere HA

**Symptoms**

Output resulting from the `bosh vms` command alternates between showing that the VMs are `failing` and showing that the VMs are `running`. The operator must run the `bosh vms` command multiple times to see this cycle.

**Explanation**

The VMs' permissions are altered during the restarting of the VM so operators have to reset permissions every time the VM reboots or is redeployed.

VMs cannot be successfully resurrected if the resurrection state of your VM is set to `off` or if the the vSphere HA restarts the VM before BOSH is aware that the VM is down.
For more information about VM resurrection, see [Resurrection](https://bosh.io/docs/sysadmin-commands/#vm-resurrection) in the Cloud Foundry BOSH documentation.

**Solution**

Run the following command on all of your master and worker VMs:

`bosh -environment BOSH-DIRECTOR-NAME -deployment DEPLOYMENT-NAME ssh INSTANCE-GROUP-NAME -c "sudo /var/vcap/jobs/kube-controller-manager/bin/pre-start; sudo /var/vcap/jobs/kube-apiserver/bin/post-start"`

Where:

+ `BOSH-DIRECTOR-NAME` is your BOSH Director name.
+ `DEPLOYMENT-NAME` is the name of your BOSH deployment.
+ `INSTANCE-GROUP-NAME` is the name of the BOSH instance group you are referencing.

The above command, when applied to each VM, gives your VMs the correct permissions.

##<a id='upgrade-drain-hangs'></a> Worker Node Hangs Indefinitely

**Symptoms**

After making your selection in the **Upgrade all clusters errand** section, the worker node might hang indefinitely.
For more information on monitoring the **Upgrade all clusters errand** using the BOSH CLI, see [Upgrade the PKS Tile](upgrade-pks.html#upgrade-tile) in _Upgrading <%= vars.product_short %>_.

**Explanation**

During the <%= vars.product_tile %> tile upgrade process, worker nodes are cordoned and drained. This drain is dependent on Kubernetes being able to unschedule all pods. If Kubernetes is unable to unschedule a pod, then the drain hangs indefinitely.
One reason why Kubernetes may be unable to unschedule the node is if the `PodDisruptionBudget` object has been configured in a way that allows 0 disruptions and only a single instance of the pod has been scheduled.

In your spec file, the `.spec.replicas` configuration sets the total amount of replicas that are available in your application.
`PodDisruptionBudget` objects can specifies the amount of replicas, proportional to that total, that must be available in your application, regardless of downtime. Operators can configure `PodDisruptionBudget` objects for each application using their spec file.

Some apps deployed using Helm-Charts may have a default `PodDisruptionBudget` set.
For more information on configuring `PodDisruptionBudget` objects using a spec file, see [Specifying a PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) in the Kubernetes documentation.

**Solution**

Configure `.spec.replicas` to be greater than the `PodDisruptionBudget` object.

When the number of replicas configured in `.spec.replicas` is greater than the number of replicas set in the `PodDisruptionBudget` object, disruptions can occur.

For more information, see [How Disruption Budgets Work](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work) in the Kubernetes documentation.
For more information about workload capacity and uptime requirements in <%= vars.product_short %>, see [Prepare to Upgrade](upgrade-pks.html#prepare) in _Upgrading <%= vars.product_short %>_.

##<a id='kubeconfig-token-expire'></a>Cannot Authenticate to an OpenID Connect-Enabled Cluster

**Symptom**

When you authenticate to an OpenID Connect-enabled cluster using an existing kubeconfig file, you see an authentication or authorization error.

**Explanation**

`users.user.auth-provider.config.id-token` and `users.user.auth-provider.config.refresh-token` contained in the kubeconfig file for the cluster may have expired.

**Solution**

1. Upgrade the PKS CLI to v1.2.0 or later.
To download the PKS CLI, navigate to [Pivotal Network](https://network.pivotal.io/products/pivotal-container-service).
For more information, see [Installing the PKS CLI](installing-pks-cli.html).

1. Obtain a kubeconfig file that contains the new tokens by running the following command:

    ```
    pks get-credentials CLUSTER-NAME
    ```
    Where `CLUSTER-NAME` is the name of your cluster.

1. Connect to the cluster using kubectl.

If you continue to see an authentication or authorization error, verify that you have sufficient access permissions for the cluster.

##<a id='bosh-pks-map'></a>Error: Failed Jobs

**Symptom**

In stdout or log files, you see an error message referencing `post-start scripts failed` or `Failed Jobs`.

**Explanation**

After deploying <%= vars.product_short %>, Ops Manager runs scripts to start a number of jobs. You must configure Ops Manager to automatically run these post-deploy scripts.

**Solution**

Perform the following steps to configure Ops Manager to run post-deploy scripts.

1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.

1. Click the **BOSH Director** tile.

1. Select **Director Config**.

1. Select **Enable Post Deploy Scripts**.
  <p class="note"><strong>Note</strong>: This setting enables post-deploy scripts for all tiles in your Ops Manager installation.</p>

1. Click **Save**.

1. Click the **Installation Dashboard** link to return to the Installation Dashboard.

1. Click **Review Pending Changes**. Review the changes that you made. For more
information, see [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/customizing/review-pending-changes.html).

1. Click **Apply Changes**.

1. (Optional) If it is a new deployment of <%= vars.product_short %>, follow the steps below:
  1. On the command line, enter `pks delete-cluster` to delete the cluster. For more information, see [Deleting Clusters](delete-cluster.html).
  1. Enter `pks create-cluster` to recreate the cluster. For more information, see [Creating Clusters](create-cluster.html).

##<a id='no-such-host'></a>Error: No Such Host

**Symptom**

In stdout or log files, you see an error message that includes `lookup vm-WORKER-NODE-GUID on IP-ADDRESS: no such host`.

**Explanation**

This error occurs on GCP when the Ops Manager Director tile uses 8.8.8.8 as the DNS server.
When this IP range is in use, the master node cannot locate the route to the worker nodes.

**Solution**

Use the Google internal DNS range, 169.254.169.254, as the DNS server.

##<a id='storage'></a>Error: FailedMount

**Symptom**

In Kubernetes log files, you see a `Warning` event from kubelet with `FailedMount` as the reason.

**Explanation**

A persistent volume fails to connect to the Kubernetes cluster worker VM.

**Diagnostics**

* In your cloud provider console, verify that volumes are being created and attached to nodes.
* From the Kubernetes cluster master node, check the controller manager logs for errors attaching persistent volumes.
* From the Kubernetes cluster worker node, check kubelet for errors attaching persistent volumes.
