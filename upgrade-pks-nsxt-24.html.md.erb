---
title: Upgrading PKS with NSX-T to NSX-T v2.4.0.1
owner: PKS
topictype: vspherewithnsxtupgrade
---

<strong><%= modified_date %></strong>

This topic describes how to upgrade your PKS with NSX-T environment from NSX-T v2.3 to v2.4.

## <a id="upgrade-prep"></a>Step 0: Prepare to Upgrade

Review related documentation in preparation for the upgrade of PKS:

1. Review the [PKS Release Notes](https://docs.pivotal.io/runtimes/pks/1-3/release-notes.html) for the supported upgrade path and known issues. 
1. Review the [VMware Product Interoperability Matrix](https://www.vmware.com/resources/compatibility/sim/interop_matrix.php) for PKS in the VMware documentation.
1. Review the [NSX-T 2.4 release notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4.0/rn/VMware-NSX-T-Data-Center-240-Release-Notes.html).

## <a id="upgrade-pks-to-136"></a> Step 1: Upgrade to PKS v1.3.6

Upgrade the PKS tile from a supported version to to PKS v1.3.6. When you upgrade the PKS tile, the target version of NCP is installed (v2.4.0 in this case). This must be done before you upgrade to NSX-T v2.4.x. 

If you are performing the upgrade during a maintenance window, it is not necessary to upgrade the Kubernetes clusters at this time, so you can deselect the upgrade all clusters errand for PKS. However, if you want your Kubernetes clusters to be upgraded immediately, ensure that the upgrade all clusters errand is enabled.
 
To upgrade the PKS tile to v1.3.6:

1. Download the PKS v1.3.6 tile from the Pivotal Network. 
2. Upload the PKS v1.3.6 tile to Ops Manager. 
3. Stage the 1.3.6 tile for deployment.
4. Review pending changes.
5. Apply changes.

## <a id="upgrade-esxi"></a> Step 2: Patch the ESXi Hosts

Upgrade all ESXi transport nodes to v6.7 EP 6. You must start at v6.7 U1 and then apply the v6.7 EP6 patch.

NSX-T v2.4.x requires vSphere v6.7 U1 and patch EP6. You will need to install vSphere v6.7 U1 first, and then upgrade to ESXi670-201901001 using vSphere Update Manager. 

Refer to the [VMware Product Interoperability Matrices](https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&175=&1=). Hover over the Information icon for vSphere 6.7 U1 and NSX-T 2.4 and you will see the following message: "VMware vSphere 6.7 EP06 (Release name: ESXi670-201901001) is the minimum supported version with NSX-T 2.4.0 (KB 2143832)."

  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-02.png" alt="Upgrade ESXi EP6 next">

For details on the ESXi v6.7 U1 EP06 patch, refer to the VMware KB article [Build numbers and versions of VMware ESXi/ESX](
https://kb.vmware.com/s/article/2143832?CoveoV2.CoveoLightningApex.getInitializationData=1&r=2&ui-communities-components-aura-components-forceCommunity-seoAssistant.SeoAssistant.getSeoData=1&other.KM_Utility.getArticleDetails=1&other.KM_Utility.getArticleMetadata=2&other.KM_Utility.getUrl=1&other.KM_Utility.getUser=1&other.KM_Utility.getAllTranslatedLanguages=2&ui-comm-runtime-components-aura-components-siteforce-qb.Quarterback.validateRoute=1).

To perform the patch upgrade using vCenter, refer to the [vSphere Upgrade Manager](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.update_manager.doc/GUID-EF6BEE4C-4583-4A8C-81B9-5B074CA2E272.html) documentation for guidance on applying the patch. See also the [VMware ESXi Upgrade](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.esxi.upgrade.doc/GUID-122035F6-8433-463E-A0F7-B4FC71A05B04.html) documentation for additional details. To patch ESXi hosts in an air-gapped environment, use Zip files as described in [the VMware ESXi documenation](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.esxi.upgrade.doc/GUID-22A4B153-CB21-47B4-974E-2E5BB8AC6874.html)

## <a id="upgrade-nsxt"></a> Step 3: Upgrade from NSX-T v2.3.1 to NSX-T v2.4

Upgrade NSX-T from v2.3.1 to v2.4.0.1. You must use at least version v2.4.0.1 due to the following known issue in v2.4.0: https://kb.vmware.com/s/article/67449.

To perform the upgrade, refer to [Upgrading NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-E04242D7-EF09-4601-8906-3FA77FBB06BD.html) in the VMware documentation.

<p class="note"><strong>Note:</strong> When upgrading NSX-T, at the stage that the ESXi Transport Nodes are upgraded ("Hosts"), you may want to create a different host group for each ESXi host in the correct order so that hosts in maintenance mode only get upgraded. In vCenter, put each EXSi Transport Node (TN) host into maintenance mode, 1 at a time. Create the host group for that ESXi host and upgrade only it, then remove it from maintenance mode. Repeat this process for all ESXi TN hosts.</p>

<p class="note"><strong>Note:</strong> Once you upgrade to NSX-T 2.4, the T0 router(s) and all other management plane objects can be seen only from the <em>Advanced Networking Configuration</em> tab. They will not be migrated to the new Policy UI.</p>

<p class="note"><strong>Note:</strong> There are architectural changes in NSX 2.4. The NSX Controller is now a component of the NSX Manager. Once the NSX-T upgrade is complete, you will have a single NSX-T Manager node. Power off the NSX Controllers. At the end of the upgrade, you can delete the NSX Controller VMs. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-D946F58F-BFB2-4F8E-A979-4C07393299E8.html">Delete NSX Controllers</a> in the NSX-T documentation.</p> 

<p class="note"><strong>Note:</strong> Once the upgrade to NSX 2.4 is complete, you may want to verify that your PKS environment is functioning properly by logging in to PKS and creating a small test cluster. If you cannot do this, troubleshoot the upgrade before proceeding. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-260D52CB-1B88-4138-A853-0293A7A3B077.html#GUID-260D52CB-1B88-4138-A853-0293A7A3B077">Troubleshooting Upgrade Failures</a> in the NSX-T documentation.</p>

## <a id="add-managers"></a> Step 4: Deploy Two Additional NSX Managers

With NSX-T v2.4, the NSX Controller component is now part of the NSX Manager. Previously the NSX Manager was a singleton, and HA was achieved using multiple NSX Controllers. With NSX-T v2.4, since the standalone NSX Controller component is no longer used, to achieve HA you need to deploy multiple (three) NSX Managers. Refer to the [Upgrading NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-3B986F37-94FE-4CAC-B4AD-9B55D8FE1EC2.html) documentation for guidance on adding additional NSX Managers.

<p class="note"><strong>Note:</strong> When you add additional NSX Managers, the system prompts you to enter a Compute Manager, which is a vCenter Server. For more information, see <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/com.vmware.nsxt.install.doc/GUID-D225CAFC-04D4-44A7-9A09-7C365AAFCA0E.html">Add a Compute Manager</a> in the NSX-T documentation.</p> 

## <a id="add-vip"></a> Step 5: Configure the NSX Manager VIP

Since you have deployed two additional NSX Managers (for a total of three), you need create a virtual IP address that can be used as a single endpoint to access the NSX Management cluster.

To create a VIP for the NSX Management cluster:

- Log in to the NSX Manager interface.
- Go to **System** > **Overview**.
- Select **Virtual IP** > **Edit**.
- Enter a publicly routable IP address, such as `10.40.206.5`.
- Click **Save**.

At this point in time, you can connect to any NSX-T manager using its own IP address, or use the VIP to connect to NSX-T Manager. Both methods work. However, note that the VIP is associated with a single NSX Manager. To determine which NSX Manager the VIP is associated with, select the Virtual IP.

  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-05.png" alt="VIP Association">

## <a id="nsx-cert"></a> Step 6: Generate and Register a New NSX Manager CA Cert with the Cluster API

Both the BOSH Director tile and the PKS tile expect the NSX Manager CA certificate. However, the current NSX Manager CA certificate is associated with the original NSX Manager IP address. You need to generate a new NSX Manager CA cert using the VIP address, then register this certificate with NSX-T using the Cluster Certificate API.

To generate a new NSX Manage CA certificate and private key using the VIP address, follow the instructions in the [Generate NSX CA Cert](./generate-nsx-ca-cert.html) PKS documentation. Make sure you use the VIP address, such as `10.40.206.5` in our example above.

Once you have created the new CA certificate using the VIP address, import the new CA certificate to the NSX Manager. Refer to [Import the Certificate to NSX Manager](./generate-nsx-ca-cert.html#import-certificate) for instructions on doing this.

Once you have imported the NSX Manager certificate, register this certificate with the NSX Management cluster using a cURL command against the Cluster Certificate API.

<p class="note"><strong>Note:</strong> In general the instructions provided in the <a href="https://docs.pivotal.io/runtimes/pks/1-3/generate-nsx-ca-cert.html#register-certificate">Register the Certificate with NSX Manager</a> documentation can be followed, with the exception that API endpoint is changed to the Cluster Certificate API.</p> 

First, create environment variables for the VIP address and the certificate ID:

```
export NSX_MANAGER_IP_ADDRESS=10.40.206.5

export CERTIFICATE_ID="63bb6646-052c-49df-b603-64d7e5bdb5bf"
```

Next, register the new NSX-T Manager CA cert using a cURL request to the Cluster Certificate API: 

```
curl --insecure -u admin:'PASSWORD' -X POST "https://$NSX_MANAGER_IP_ADDRESS/api/v1/cluster/api-certificate?action=set_cluster_certificate&certificate_id=$CERTIFICATE_ID"
```

The result is a `certificate_id` that is returned:

```
{
Â  "certificate_id": "63bb6646-052c-49df-b603-64d7e5bdb5bf"
}
```

To verify, using a browser go to the VIP address of the NSX Manager. Login and check that the new cert is used by the site (accessed using the VIP address). 

To further verify, SSH to each NSX Manager host and run the following two commands. All certificates returned should be the same.

```
get certificate api
get certificate cluster
```

## <a id="upgrade-tiles"></a> Step 7: Update PKS and BOSH with New NSX Manager Cert and VIP

The last procedure in the upgrade process is to modify the BOSH Tile and the PKS Tile with the new VIP address for the NSX Manager and the new NSX-T Manager CA cert (using VIP info). Apply the changes and ensure that the **Upgrade all clusters errand** is selected, then deploy PKS.

To update the BOSH tile:

1. Log into Ops Manager. 
1. In the BOSH Director tile, select the **vCenter Configuration** tab.
1. In the **NSX Address** field, enter the VIP address for the NSX Management Cluster.
1. In the **NSX CA Cert** field, enter the new CA certificate for the NSX Management Cluster that uses the VIP address.
1. Save the BOSH tile changes.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-08.png" alt="Update BOSH with VIP and Cert"> 

To update the PKS tile:

1. Log into Ops Manager. 
1. In the PKS tile, select the **Networking** tab.
1. In the **NSX Manager hostname** field, enter the VIP address for the NSX Management Cluster.
1. In the **NSX Manager CA Cert** field, enter the new CA certificate for the NSX Management Cluster (that uses the VIP address).
1. Save the PKS tile changes.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-09.png" alt="Update PKS with VIP and Cert"> 

## <a id="upgrade-k8s"></a> Step 8: Upgrade all Kubernetes Clusters

Once you have updated the PKS and BOSH tiles, apply the changes. Be sure to run the "Upgrade all [Kubernetes] clusters errand". Doing so will allow NCP configurations on all Kubernetes clusters to be updated with the new NSX-T Management Cluster VIP and CA certificate.

To complete the upgrade:

1. Go to the **Installation Dashboard** in Ops Manager.
1. Click **Review Pending Changes.**
1. Expand the **Errands** list for PKS.
1. Ensure that the **Upgrade all clusters errand** is selected.
1. Click **Apply Changes**.
  <img src="images/nsxt/nsx-upgrade/nsx_24_upgrade-10.png" alt="Upgrade all Kubernetes clusters"> 

## <a id="verify-ncp"></a> Step 9: Verify PKS Upgrade

Once the upgrade is complete, verify that NCP configuration is automatically updated with the new VIP (instead of individual NSX-T Manager node IP address).

To do this, run a command similar to the following for each Kubernetes cluster (service-instance_UUID):

```
bosh ssh master/0 -d service-instance_d9b662d0-23e1-4239-b641-ed20ee62e692
```

Note the "nsx_api_managers" address. It should be the VIP.


## <a id="update-clis"></a> Step 10: Update PKS and Kubernetes CLIs

Update the PKS and Kubernetes CLIs on any local machine 
where you run commands that interact with your upgraded version of PKS.

To update your CLIs, download and re-install the PKS and Kubernetes CLI distributions 
that are provided with PKS on Pivotal Network.

For more information about installing the CLIs, see the following topics:

* [Installing the PKS CLI](installing-pks-cli.html)

* [Installing the Kubernetes CLI](installing-kubectl-cli.html)
