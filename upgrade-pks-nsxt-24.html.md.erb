---
title: Upgrading PKS with NSX-T to NSX-T v2.4.0.1
owner: PKS
topictype: vspherewithnsxtupgrade
---

<strong><%= modified_date %></strong>

This topic describes how to upgrade your PKS with NSX-T evnvironment from NSX-T v2.3 to v2.4.

## <a id="upgrade-path"></a> Supported Upgrade Path

Upgrading PKS with NSX-T 2.3 to use NSX-T 2.4 is a two phase process:

- Phase 1: Upgrade the PKS tile from v1.3.4 or v.1.3.5 to PKS v1.3.6.
- Phase 2: Upgrade from NSX-T v2.3.1 to NSX-T v2.4.0.1. 

<p class="note"><strong>Note</strong>: You must upgrade to PKS v1.3.6 from v1.3.4 or later. No other upgrade path is supported. If you are running PKS 1.3.3 or earlier, you must upgrade to PKS 1.3.4 or v1.3.5 so that you are on NCP 2.3.2. Only from there can you upgrade to PKS 1.3.6/NCP 2.4.0.</p>

## <a id="upgrade-prep"></a>Step 0: Prepare to Upgrade

Review related documenation and the high-level upgrade steps so you are familiar with the upgrade workflow.

1. Review the [VMware Product Interoperability Matrix](https://www.vmware.com/resources/compatibility/sim/interop_matrix.php) for PKS in the VMware documentation.
1. Review the [NSX-T 2.4 release notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4.0/rn/VMware-NSX-T-Data-Center-240-Release-Notes.html).
1. Verify the [supported upgrade path](#upgrade-path). 
1. Upgrade the PKS tile from 1.2.8 or later or 1.3.0 or later to PKS v1.3.4 or PKS v1.3.5, both of which include NCP v2.3.2 which is required to upgrade to NCP 2.4.
1. Upgrade PKS tile from v1.3.4 or v1.3.5 to v1.3.6. 
1. Upgrade all ESXi transport nodes to v6.7 EP 6. You must start at v6.7 U1 and then apply the v6.7 EP6 patch.
1. Upgrade NSX-T from v2.3.1 to v2.4.0.1. You must use version v2.4.0.1 due to the following known issue in v2.4.0: https://kb.vmware.com/s/article/67449.
1. Power off the legacy controllers.
1. Increase the size of the NSX Manager disk to 200GB.
1. Run NSX-T upgrad post check and verify that everything works as expected with 1 NSX Manager appliance.
1. Deploy two additional NSX Manager nodes to form a cluster of three NSX Manager nodes for high-availability. (Optional by recommended.)
1. Configure the NSX Manager VIP. (Required if you added multiple NSX Managers as recommended.)
1. Generate a new NSX Manager CA cert using the VIP address, and register this cert to the NSX Manager node cluster (using curl command against the cluster_certificate API).
1. Modify BOSH Tile and PKS Tile with the new NSX-T Mgr CA cert (using VIP info) and the new IP Address for NSX-T Mgr (using VIP now), then apply changes.
1. Upgrade all Kubernetes clusters using the upgrade Kubernetes clusters errand.
1. Verify that NCP configuration is automatically updated with the new VIP (instead of individual NSX-T Mgr node IP address).

## <a id="upgrade-pks-to-134-5"></a> Step 1: Upgrade to PKS v1.3.4 or v1.3.5

You must be on PKS v1.3.4 or v1.3.5 to upgrade to PKS v.1.3.6. No other upgrade path is supported. Skip this step if you are already running PKS v1.3.4 or v1.3.5. When you upgrade to PKS v1.3.4 or v1.3.5, NCP v2.3.2 is installed. This version is required to upgrade to NCP v2.4.

During the upgrade , ensure that the following errands are enabled:
- NSX-T Validation errand for PKS
- Upgrade all clusters errand for PKS

## <a id="upgrade-pks-to-136"></a> Step 2: Upgrade from PKS v1.3.4 or v1.3.5 to PKS v1.3.6

Upgrade the PKS tile from v1.3.4 or v1.3.5 to PKS v1.3.6. When you upgrade the PKS tile to v1.3.6, NCP v2.4.0 is installed. This must be done before you upgrade to NSX-T v2.4.0.1. It is not necessary to upgrade the Kubernetes clusters at this time, so you can deselect the upgrade all clusters errand for PKS.
 
To upgrade the PKS tile from v1.3.4 or v1.3.5 to v1.3.6:

1. Download the PKS v1.3.6 tile from the Pivotal Network. 
2. Upload the PKS v1.3.6 tile to Ops Manager. 
3. Stage the 1.3.6 tile for deployment.
4. Review pending changes.
5. Apply changes.

## <a id="upgrade-esxi"></a> Step 3: Patch the ESXi Hosts

NSX-T v2.4.0.1 requires vSphere v6.7 U1 and patch EP6. You will need to install vSphere v6.7 U1 first, and then upgrade to ESXi670-201901001 using vSphere Update Manager. 

Refer to the VMware Interop Matrix: https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&175=&1=. Hover over the Information icon for vSphere 6.7 U1 and NSX-T 2.4 and you will see that "VMware vSphere 6.7 EP06 (Release name: ESXi670-201901001) is the minimum supported version with NSX-T 2.4.0 (KB 2143832)."

  <img src="images/nsx_24_upgrade-02.png" alt="Upgrade ESXi EP6 next">

For details on the ESXi v6.7 EP06 patch, refer to the VMware KB article [Build numbers and versions of VMware ESXi/ESX](
https://kb.vmware.com/s/article/2143832?CoveoV2.CoveoLightningApex.getInitializationData=1&r=2&ui-communities-components-aura-components-forceCommunity-seoAssistant.SeoAssistant.getSeoData=1&other.KM_Utility.getArticleDetails=1&other.KM_Utility.getArticleMetadata=2&other.KM_Utility.getUrl=1&other.KM_Utility.getUser=1&other.KM_Utility.getAllTranslatedLanguages=2&ui-comm-runtime-components-aura-components-siteforce-qb.Quarterback.validateRoute=1).

To perform the patch upgrade using vCenter:
1. Log in to your vCenter Sever using the vSphere Client.
1. Go to the **Update Manager**.
1. Select the **Updates** tab.
1. Select the "Non-Critical Host Patches." 
1. Locate the minimum EP6 patch in the list and proceed with the upgrade. Refer to the [vSphere Upgrade Manager](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.update_manager.doc/GUID-EF6BEE4C-4583-4A8C-81B9-5B074CA2E272.html) documentation for guidance on applying the patch.
  <img src="images/nsx_24_upgrade-03.png" alt="Upgrade ESXi EP6">

Refer to the [VMware ESXi Upgrade](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.esxi.upgrade.doc/GUID-122035F6-8433-463E-A0F7-B4FC71A05B04.html) documentation for additional details. To patch ESXi hosts in an air-gapped environment, use Zip files as described in [the documenation](https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.esxi.upgrade.doc/GUID-22A4B153-CB21-47B4-974E-2E5BB8AC6874.html)

## <a id="upgrade-nsxt"></a> Step 4: Upgrade from NSX-T v2.3.1 to NSX-T v2.4.0.1

Complete the following steps to upgrade from NSX-T 2.3.1 to NSX-T v2.4.0.1. For detailed guidance, refer to [Upgrading NSX-T Data Center](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/upgrade/GUID-E04242D7-EF09-4601-8906-3FA77FBB06BD.html) in the VMware documentation.

1. Log in to the VMware software download site and download the NSX-T 2.4.0.1 upgrade bundle file: `VMware-NSX-upgrade-bundle-2.4.0.1.0.<Build-ID>.mub`. 
1. Log in to the NSX Manager web interface.
1. Go to **System** > **Utilities**. 
1. Select the **Upgrade** tab.
1. Click **Proceed to Upgrade**.
1. Scroll down and select **Upload MUB file**. 
1. Browse to and select the upgrade bundle file: `VMware-NSX-upgrade-bundle-2.4.0.1.0.<Build-ID>.mub`.
1. Click **Open**.
1. Click **Upload**. The upload process begins and will take several minutes.
1. Once the MUB file is successfully uploaded, click **Begin Upgrade**.
  <img src="images/nsx_24_upgrade-04.png" alt="Begin Upgrade">
1. Accept the license and click **Continue**. 
1. Click **Next**.
1. Click **Run Pre-Checks**. You will see a warning for at least the NSX Manager server. Because of the architectural changes to NSX Manager, make sure you reivew the warning and have sufficient resources for the new NSX Manager server (6 CPU and 24 GB of RAM).
1. Complete the upgrade by following the upgrade wizard. 

<p class="note"><strong>Note:</strong> When upgrading NSX-T, at the stage that the ESXi Transport Nodes are upgraded ("Hosts"), make sure to create different a different host group for each ESXi host in the correct order so that hosts in maintenance mode only get upgraded. In vCenter, put each EXSi TN host into maintenance mode, 1 at a time. Create the host group for that ESXi host and upgrade only it, then remove it from maintenance mode. Repeat this process for all ESXi TN hosts.</p>

## <a id="remove-controllers"></a> Step 5: Power Off the NSX Controllers

There are architectural changes in NSX 2.4. The NSX Controller is now a component of the NSX Manager. Once the NSX-T upgrade is complete, you will have a single NSX-T Manager node. Power off the NSX Controllers. At the end of the upgrade, you can delete the controller VMs.

<p class="note"><strong>Note:</strong> Once you upgrade to NSX-T 2.4, the T0 router(s) and all other management plane objects can be seen only from the "Advanced Networking Configuration" tab. They will not be migrated to the new policy UI.</p>

## <a id="add-manager-capacity"></a> Step 6: Verify PKS Functionality

Once the upgrade to NSX 2.4 is complete, verify that your PKS environment is functioning properly by logging to to PKS and creating a small test cluster.

## <a id="add-managers"></a> Step 7: Deploy Two Additional NSX Managers

With NSX-T v2.4, the NSX Controller component is now part of the NSX Manager. Previously the NSX Manager was a singleton, and HA was achieved using multiple NSX Controllers. With NSX-T v2.4, since the NSX Controller component is no longer used, to achieve HA you need to deploy multiple (three) NSX Managers.

<p class="note"><strong>Note:</strong> When you add additional NSX Managers, the system prompts you to enter a Compute Manager, which is a vCenter Server. If you do not have a Compute Manager configured, create one now before proceeding. See the [Add a Compute Manager](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.3/com.vmware.nsxt.install.doc/GUID-D225CAFC-04D4-44A7-9A09-7C365AAFCA0E.html) in the NSX-T documentation.</p>

To add an NSX Manager:

- Log in to the NSX Manager web interface.
- Select the **Home** tab.
- Click **Add Nodes**.
- Configure the host and node fields.

When the new NSX Manager is added, the system will sync it. When the sync is complete and successful, you should see that the **Repository Status** is "Sync Completed".

Repeat the process so that you have 3 NSX Managers.

## <a id="add-vip"></a> Step 8: Configure the NSX Manager VIP

Assuming you deployed two additional NSX Managers (such that you have three), you need create a virtual IP address that can be used to access the NSX Manager web interface.

- Go to System > Overview.
- Select Virtual IP > Edit.
- Enter a publically routable virtual IP address.
- Click Save.

At this point in time, you can connect to any NSX-T manager using its own IP address or use the VIP to connect to NSX-T Manager. Both methods work. However, note that the VIP will be associated with a single NSX Manager. To determine which NSX Manager the VIP is associated with, select the Virtual IP.

  <img src="images/nsx_24_upgrade-05.png" alt="VIP Association">

## <a id="nsx-cert"></a> Step 9: Generate and Register a New NSX Manager CA Cert for the VIP

Create a new NSX Manager CA cert using the VIP address, and register this certificate with the NSX Manager node cluster using a cURL command against the cluster_certificate API.

1. Generate the certificate. Refer to [Generate NSX CA Cert](https://docs.pivotal.io/runtimes/pks/1-3/generate-nsx-ca-cert.html). Be sure to use the VIP address.
1. Import the cert to the VIP NSX Manager. Refer to [Import the Certificate to NSX Manager](https://docs.pivotal.io/runtimes/pks/1-3/generate-nsx-ca-cert.html#import-certificate).
1. Register the cert using the NSX API. You must register the cert with all three NSX Manager hosts using a cURL request to the Cluster Certificate API. 

For example:

```
# export NSX_MANAGER_IP_ADDRESS=192.0.2.0

# export CERTIFICATE_ID="63bb6646-052c-49df-b603-64d7e5bdb5bf"

# curl --insecure -u admin:'VMware1!' -X POST "https://$NSX_MANAGER_IP_ADDRESS/api/v1/cluster/api-certificate?action=set_cluster_certificate&certificate_id=$CERTIFICATE_ID"

{
  "certificate_id": "63bb6646-052c-49df-b603-64d7e5bdb5bf"
}
```

To verify, using a browser go to the VIP address of the NSX Manager. Login and check that the new cert is used by the site (accessed using the VIP address). 

To further verify, SSH to each NSX Manager host and run the following two commands:

```
get certificate api
get certificate cluster
```

All certificates returned should be the same.

## <a id="upgrade-tiles"></a> Step 10: Update PKS and BOSH with New NSX Manager Cert and VIP

1. Log into Ops Manager. 
1. In the BOSH Director tile, select the vCenter Configuration tab.
1. In the NSX Address field, enter the VIP address.
1. In the NSX CA Cert field enter the new cluster certificate CA cert (using the VIP address). 
1. Save the BOSH tile changes.
1. In the PKS tile, select the Networking tab.
1. In the NSX Manager hostname field, enter the VIP address.
1. In the NSX Manager CA Cert field, enter the new cluster certificate CA cert (using the VIP address).
1. Save the PKS tile changes.

## <a id="upgrade-k8s"></a> Step 11: Upgrade all Kubernetes Clusters

Once you have updated the PKS and BOSH tiles, apply the changees. Be sure to run the "Upgrde all [Kubernetes] clusters errand". Doing so will allow NCP configurations on all Kubernetes clusters to be updated with the bew NSX-T MAnager VIP, instead of the individual NSX-T Mgr node IP as set before the upgrade.

## <a id="verify-ncp"></a> Step 12: Verify NCP Configuration

Verify that NCP configuration is automatically updated with the new VIP (instead of individual NSX-T Mgr node IP address).

To do this, run a command similar to the following for each Kubernetes cluster (service instance):

```
bosh ssh master/0 -d service-instance_d9b662d0-23e1-4239-b641-ed20ee62e692
```

Note the "nsx_api_managers" address. It should be the VIP.

## <a id="update-clis"></a> Step 13: Update PKS and Kubernetes CLIs

Update the PKS and Kubernetes CLIs on any local machine 
where you run commands that interact with your upgraded version of PKS.

To update your CLIs, download and re-install the PKS and Kubernetes CLI distributions 
that are provided with PKS on Pivotal Network.

For more information about installing the CLIs, see the following topics:

* [Installing the PKS CLI](installing-pks-cli.html)

* [Installing the Kubernetes CLI](installing-kubectl-cli.html)

### <a id="verify"></a> Step 14: Verify the Upgrade

To verify successful upgrade, create a new Kubernetes test cluster. For example:

```
pks create-cluster pks-cluster-medium-LB-5 --external-hostname k8s-cluster-medium-LB-5 --plan large --num-nodes 6 --network-profile network-profile-medium
```

## <a id="add-manager-capacity"></a> Step 6: Upgrade NSX Manager Disk Size

Because the controller component is now part of NSX Manager, the NSX Manager requires more disk space. Upgrade NSX-T Mgr appliance disk to 200GB.