---
title: Cloud Native Storage (CNS) on vSphere
owner: PKS
---

This topic explains how to integrate Cloud Native Storage (CNS) with <%= vars.product_full %> with a Cloud Storage Interface (CSI).
This integration enables <%= vars.k8s_runtime_abbr %> clusters to use external container storage.

CNS makes external storage for Kubernetes clusters fully automated, scalable, and visible within vCenter.
For more information, see [Introducing Cloud Native Storage for vSphere](https://blogs.vmware.com/virtualblocks/2019/08/14/introducing-cloud-native-storage-for-vsphere/) in the VMware _Virtual Blocks_ blog.

## <a id=''></a>Terminology
    
- VCP : In-Tree cloud provider that contains storage bits to
  provision PVC into vSphere disks
- CSI : External container storage Interface for K8s clusters

## <a id=''></a>Prerequisites for CNS with PKS
    
- IaaS

  - [vSphere](https://cloud-provider-vsphere.sigs.k8s.io/glossary.html#vsphere "vSphere is the product name of the two core components of the VMware Software Defined Datacenter (SDDC) stack, they are vCenter and ESXi. Each is discussed below in detail.") 6.7U3
    (or later) is a prerequisite for using CSI
    : <https://buildweb.eng.vmware.com/ob/14367737/>  and 
     <https://buildweb.eng.vmware.com/ob/14320388/>
  - As a result, NSX-T that can be used on such an IaaS is 2.4.0
    and above
    : <https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&1=&175=>
    
- Firewall & Network Requirements
      
  - All PKS k8s cluster worker VM will need access to VC
    Management Plane. VCP might only require master node's
    access but since CSI components are installed on k8s
    workers, workers too will need access to Management plane to
    provision disks
  - Additionally all pods running CSI components will need
    access to management plane through pod network: 

    - PODs in kube-system namespace MUST be able to access vCenter

      Once CSI is installed in the K8s cluster, you will see these PODs in the kube-system namespace:

      <pre class="terminal">
      **kubectl
      get pod -n kube-system -o wide**  
      NAME                                    READY   STATUS    RESTARTS  
      AGE     IP             NODE                                   NOMINATED
      NODE   READINESS GATES  
      vsphere-csi-controller-0                5/5     Running   5         
      24h     172.16.30.3    c4e3819f-00fc-457b-beda-26fbdd589c53  
      \<none\>           \<none  
      vsphere-csi-node-64dmx                  3/3     Running   3         
      24h     172.16.30.9    c9b4f441-4c08-43cf-bb17-8be80ed676a4  
      \<none\>           \<none\>  
      vsphere-csi-node-6bp4x                  3/3     Running   0         
      5h15m   172.16.30.13   80dc0ceb-d460-4538-99b6-d7d8eacc4b74  
      \<none\>           \<none\>  
      vsphere-csi-node-92tvh                  3/3     Running   3         
      5h21m   172.16.30.11   651bfb0d-084d-481f-98f6-f811284676ef  
      \<none\>           \<none\>  
      vsphere-csi-node-l4v2q                  3/3     Running   0         
      4h33m   172.16.30.17   9233bd53-7e94-4b94-a197-f459ccdc25dd  
      \<none\>           \<none\>  
      vsphere-csi-node-nnldx                  3/3     Running   3         
      24h     172.16.30.8    c4e3819f-00fc-457b-beda-26fbdd589c53  
      \<none\>           \<none\>  
      vsphere-csi-node-pw7hw                  3/3     Running   3         
      5h18m   172.16.30.12   1b84ae77-45bf-4fa1-9f0b-cbc13bb75894  
      \<none\>           \<none\> 
      </pre> 
          All these PODs must be able to access vCenter.

          What is means is the Floating IP allocated to the SNAT rule for this
          namespace in the T0 (or T1 if shared T1 model is used) MUST be able to
          reach vCenter.

- PKS Version
    
  - PKS 1.7+  (support upgrading virtual hardware version on K8S
            cluster VMs)

- PKS Clusters/Plan 
    
  - Privileged mode allowed on containers (Why? Tushar to find
            out and update. )

## <a id=''></a>Manually Installing CSI on a PKS Cluster

There are two ways to deploy CSI with Ent PKS - 1) Manually deploying. 
    
### <a id=''></a>Step 1: create a PKS cluster

```
pks create-cluster pks-cluster-5-shared-t1 --external-hostname
pks-cluster-5-shared-t1 --plan large --num-nodes 3 --network-profile
single-tier-profile
```

### <a id=''></a>Step 2: Create a CSI secret

Create the following file anywhere in your system:

  ```
  csi-vsphere.conf
  \[Global\]
  cluster-id = "pks-cluster-5-shared-t1"

  \[VirtualCenter "10.1.1.1"\]
  insecure-flag = "true"
  user = "administrator@vsphere.local"
  password = "VMware1\!"
  port = "443"
  datacenters = "vSAN\_Datacenter"
  ```

<pre class="terminal">
\> kubectl create secret generic vsphere-config-secret
--from-file=csi-vsphere.conf --namespace=kube-system
secret/vsphere-config-secret created  
</pre>
  
Check:  
<pre class="terminal">
\> kubectl
get secret/vsphere-config-secret -n kube-system**  
NAME                    TYPE     DATA   AGE  
vsphere-config-secret   Opaque   1      37s
</pre>

### <a id=''></a>Step 3: Create Roles, ServiceAccount and ClusterRoleBinding

Use the manifest below:

```
kind: ServiceAccount
apiVersion: v1
metadata:
  name: vsphere-csi-controller
  namespace: kube-system
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-controller-role
rules:
  - apiGroups: [""]
    resources: ["nodes", "persistentvolumeclaims", "pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vsphere-csi-controller-binding
subjects:
  - kind: ServiceAccount
    name: vsphere-csi-controller
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: vsphere-csi-controller-role
  apiGroup: rbac.authorization.k8s.io
```

<pre class="terminal">
\> kubectl apply -f vsphere-csi-controller-rbac.yaml  
serviceaccount/vsphere-csi-controller created  
[*clusterrole.rbac.authorization.k8s.io/vsphere-csi-controller-role*](http://clusterrole.rbac.authorization.k8s.io/vsphere-csi-controller-role) created  
[*clusterrolebinding.rbac.authorization.k8s.io/vsphere-csi-controller-binding*](http://clusterrolebinding.rbac.authorization.k8s.io/vsphere-csi-controller-binding) created
</pre>

### <a id=''></a>Step 4: Install the vSphere CSI driver

[*vsphere-csi-controller-ss-data.yaml*](https://confluence.eng.vmware.com/download/attachments/586132395/vsphere-csi-controller-ss-data.yaml?version=1&modificationDate=1583374769000&api=v2) (Not
a final YAML)

<pre class="terminal">
\> kubectl apply -f vsphere-csi-controller-ss-data-1.yaml  
statefulset.apps/vsphere-csi-controller created  
[*csidriver.storage.k8s.io/csi.vsphere.vmware.com*](http://csidriver.storage.k8s.io/csi.vsphere.vmware.com) created  



[*vsphere-csi-node-ds-data.yaml*](https://confluence.eng.vmware.com/download/attachments/586132395/vsphere-csi-node-ds-data.yaml?version=1&modificationDate=1583374769000&api=v2)

\> kubectl apply -f vsphere-csi-node-ds-data.yaml 
daemonset.apps/vsphere-csi-node created
</pre>

### <a id=''></a>Step 5: Verify that CSI has been successfully deployed

<pre class="terminal">
\> kubectl get statefulset --namespace=kube-system 
  
NAME                     READY   AGE

\> kubectl get daemonsets vsphere-csi-node --namespace=kube-system  
NAME               DESIRED   CURRENT   READY   UP-TO-DATE  
AVAILABLE   NODE SELECTOR   AGE  
vsphere-csi-node   3         3         3       3           
3           \<none\>          13m

\> kubectl get pods --namespace=kube-system  
NAME                                    READY   STATUS    RESTARTS  
AGE  
coredns-6f9bcd8956-9x7wn                1/1     Running   0         
22h  
coredns-6f9bcd8956-clksx                1/1     Running   0         
53m  
coredns-6f9bcd8956-rjmfl                1/1     Running   0         
22h  
kubernetes-dashboard-5fc4ccc79f-kwnd4   1/1     Running   0         
22h  
metrics-server-7f85c59675-vz4bs         1/1     Running   0         
22h  
vsphere-csi-controller-0                5/5     Running   0         
14m  
vsphere-csi-node-64dmx                  3/3     Running   0         
13m  
vsphere-csi-node-nnldx                  3/3     Running   0         
13m  
vsphere-csi-node-pdbb5                  3/3     Running   0         
13m
</pre>

### <a id=''></a>Step 6: Verify that the CSI Custom Resource Definitions are working

<pre class="terminal">
\> kubectl get CSINode 
NAME                                   CREATED AT  
3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b   2020-03-05T22:29:00Z  
c4e3819f-00fc-457b-beda-26fbdd589c53   2020-03-05T22:28:57Z  
c9b4f441-4c08-43cf-bb17-8be80ed676a4   2020-03-05T22:29:00Z

\> kubectl describe CSINode  
Name:         3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b  
Namespace:      
Labels:       \<none\>  
Annotations:  \<none\>  
API
Version:  [*storage.k8s.io/v1beta1*](http://storage.k8s.io/v1beta1)  
Kind:         CSINode  
Metadata:  
  Creation Timestamp:  2020-03-05T22:29:00Z  
  Owner References:  
    API Version:     v1  
    Kind:            Node  
    Name:            3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b  
    UID:             2ab6f1cb-a2f7-41a9-87b8-4197177c6b70  
  Resource Version:  153666  
  Self Link:        
/apis/[*storage.k8s.io/v1beta1/csinodes/3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b*](http://storage.k8s.io/v1beta1/csinodes/3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b)  
  UID:               79144892-45b3-4fa7-8242-a2468993260a  
Spec:  
  Drivers:  
   
Name:           [*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/)  
    Node ID:        3a0ea98b-879e-4f7a-abbd-e3ad426c8a1b  
    Topology Keys:  \<nil\>  
Events:             \<none\>  

Name:         c4e3819f-00fc-457b-beda-26fbdd589c53  
Namespace:      
Labels:       \<none\>  
Annotations:  \<none\>  
API
Version:  [*storage.k8s.io/v1beta1*](http://storage.k8s.io/v1beta1)  
Kind:         CSINode  
Metadata:  
  Creation Timestamp:  2020-03-05T22:28:57Z  
  Owner References:  
    API Version:     v1  
    Kind:            Node  
    Name:            c4e3819f-00fc-457b-beda-26fbdd589c53  
    UID:             442cdc30-2e2b-4e7f-ac2b-17e667f18688  
  Resource Version:  153646  
  Self Link:        
/apis/[*storage.k8s.io/v1beta1/csinodes/c4e3819f-00fc-457b-beda-26fbdd589c53*](http://storage.k8s.io/v1beta1/csinodes/c4e3819f-00fc-457b-beda-26fbdd589c53)  
  UID:               acbc421b-cf7f-415d-bc10-1316b28dbd47  
Spec:  
  Drivers:  
   
Name:           [*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/ 
    Node ID:        c4e3819f-00fc-457b-beda-26fbdd589c53  
    Topology Keys:  \<nil\>  
Events:             \<none\>  

Name:         c9b4f441-4c08-43cf-bb17-8be80ed676a4  
Namespace:      
Labels:       \<none\>  
Annotations:  \<none\>  
API
Version:  [*storage.k8s.io/v1beta1*](http://storage.k8s.io/v1beta1)  
Kind:         CSINode  
Metadata:  
  Creation Timestamp:  2020-03-05T22:29:00Z  
  Owner References:  
    API Version:     v1  
    Kind:            Node  
    Name:            c9b4f441-4c08-43cf-bb17-8be80ed676a4  
    UID:             c4d8e053-8a3f-466e-bd1f-d491f58cabc8  
  Resource Version:  153663  
  Self Link:        
/apis/[*storage.k8s.io/v1beta1/csinodes/c9b4f441-4c08-43cf-bb17-8be80ed676a4*](http://storage.k8s.io/v1beta1/csinodes/c9b4f441-4c08-43cf-bb17-8be80ed676a4)  
  UID:               69958fca-95ac-4e70-b690-af81c2434fc5  
Spec:  
  Drivers:  
   
Name:           [*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/)  
    Node ID:        c9b4f441-4c08-43cf-bb17-8be80ed676a4  
    Topology Keys:  \<nil\>  
Events:             \<none\>
</pre>

<pre class="terminal">
\> kubectl get csidrivers
NAME                     CREATED AT  
[*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/)  
2020-03-05T22:28:21Z

\> kubectl describe csidrivers

Name:         [*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/)  
Namespace:      
Labels:       \<none\>  
Annotations:  [*kubectl.kubernetes.io/last-applied-configuration*](http://kubectl.kubernetes.io/last-applied-configuration):  
               
{"apiVersion":"storage.k8s.io/v1beta1","kind":"CSIDriver","metadata":{"annotations":{},"name":"csi.vsphere.vmware.com"},"spec":{"attachReq...  
API
Version:  [*storage.k8s.io/v1beta1*](http://storage.k8s.io/v1beta1)  
Kind:         CSIDriver  
Metadata:  
  Creation Timestamp:  2020-03-05T22:28:21Z  
  Resource Version:    153505  
  Self Link:          
/apis/[*storage.k8s.io/v1beta1/csidrivers/csi.vsphere.vmware.com*](http://storage.k8s.io/v1beta1/csidrivers/csi.vsphere.vmware.com)  
  UID:                 906e512c-c897-40cb-8c97-9975fce2fcf8  
Spec:  
  Attach Required:    true  
  Pod Info On Mount:  false  
Events:               \<none\>
</pre>

### <a id=''></a>Step 7: Verify ProviderID has been added the nodes
    
<pre class="terminal">  
  \> kubectl describe nodes | grep "ProviderID"**  
  ProviderID:                  [*vsphere://421025c3-0ce4-8cff-8229-1a2ec0bf2d97*](vsphere://421025c3-0ce4-8cff-8229-1a2ec0bf2d97)  
  ProviderID:                  [*vsphere://42109234-71ec-3f26-5ddd-9c97c5a02fe9*](vsphere://42109234-71ec-3f26-5ddd-9c97c5a02fe9)  
  ProviderID:                  [*vsphere://4210ecc1-e7d8-a130-19e5-f20804b5b36e*](vsphere://4210ecc1-e7d8-a130-19e5-f20804b5b36e)  
</pre>

### <a id=''></a>Step 8: Create a vSphere storage class
    
Create the following YAML:

  ```
  cassandra-storageclass.yaml
  apiVersion: [*storage.k8s.io/v1*](http://storage.k8s.io/v1)  
  kind: StorageClass  
  metadata:  
    name: demo-sts-sc  
    annotations:  
      [*storageclass.kubernetes.io/is-default-class*](http://storageclass.kubernetes.io/is-default-class):
  "true"  
  provisioner: [*csi.vsphere.vmware.com*](http://csi.vsphere.vmware.com/)  
  parameters:  
    datastoreurl:
  "[*ds:///vmfs/volumes/vsan:52d8eb4842dbf493-41523be9cd4ff7b7/*](ds://confluence.eng.vmware.com/vmfs/volumes/vsan:52d8eb4842dbf493-41523be9cd4ff7b7/)"
  ```

Note: the datastoreurl can be found here:

## <a id=''></a>Automated Installation of CSI on PKS Clusters

To automate the deployment of CSI on the K8s cluster at creation
    time, we can use the PKS Plan Adds-on capability.

Follow the steps below:

  - obtain `csi-vsphere.conf` by running the command:
  `cat csi-vsphere.conf | base64 -w0`
  
  - create the secret YAML file:

  ```
  secret.yaml
  apiVersion: v1    
  kind: Secret
  metadata:
    name: vsphere-config-secret
    namespace: kube-system
  type: Opaque
  data:
  csi-vsphere.conf: W0dsb2JhbF0KY2x1c3Rlci1pZCA9ICJwa3MtY2x1c3Rlci01LXNoYXJlZC10MSIKCltWaXJ0dWFsQ2VudGVyICIxMC4xOTkuMT........
  ```

  - select a PKS Plan and populate the Adds-on field with all the
        YAML:

in the adds-on field, copy/paste the following YAML files (separated by
'—'):

    - secret.yaml
    - vsphere-csi-controller-rbac.yaml
    - vsphere-csi-controller-ss-data-1.yaml
    - vsphere-csi-node-ds-data.yaml

This should give something like:

[*pks-plan-addson.yaml*](https://confluence.eng.vmware.com/download/attachments/586132395/pks-plan-addson.yaml?version=1&modificationDate=1583949395000&api=v2)

  Note: If needed, you can even add the Storage Class YAML (the one that will be
    used as default for CSI - for instance cassandra-storageclass.yaml, the
    one used in this page) in this PKS plan adds-on field.

That's all. Apply the changes on Ops mgr and then deploy a new K8s
cluster using this PKS plan.

For instance, the PKS plan I created was named 'plan-csi' and the way to
create a K8s cluster using this plan is:

`pks create-cluster pks-cluster-1-shared-t1i-csi --external-hostname
pks-cluster-1-shared-t1-csi --plan plan-csi --num-nodes 3
--network-profile single-tier-profile`
